[{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement b-cubedsupport@meisebotanicgarden.. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"CONTRIBUTING","text":"Small typos grammatical errors documentation may edited directly using GitHub web interface, long changes made source file. E.g. edit roxygen2 comment .R file R/, .Rd file man/.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CONTRIBUTING.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"CONTRIBUTING","text":"make substantial pull request, always file issue make sure someone team agrees ’s problem. ’ve found bug, create associated issue illustrate bug minimal reproducible example.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"","what":"Pull request process","title":"CONTRIBUTING","text":"recommend create Git branch pull request (PR). Look GitHub Actions build status making changes. README contain badges continuous integration services used package. require tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2. use testthat. Contributions test cases included easier accept. user-facing changes, add bullet top NEWS.md current development version header describing changes made followed GitHub username, links relevant issue(s)/PR(s).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"CONTRIBUTING","text":"Please note project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CONTRIBUTING.html","id":"prefer-to-email","dir":"","previous_headings":"","what":"Prefer to Email?","title":"CONTRIBUTING","text":"Email person listed maintainer DESCRIPTION file repo. Though note private discussions email don’t help others - course email totally warranted ’s sensitive problem kind.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/CONTRIBUTING.html","id":"thanks-for-contributing","dir":"","previous_headings":"","what":"Thanks for contributing!","title":"CONTRIBUTING","text":"contributing guide adapted tidyverse contributing guide available https://raw.githubusercontent.com/r-lib/usethis/master/inst/templates/tidy-contributing.md","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Research Institute Nature Forest (INBO) Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Calculating Bootstrap Confidence Intervals","text":"working data cubes, ’s essential understand uncertainty surrounding derived statistics. tutorial introduces calculate_bootstrap_ci() function dubicube, uses bootstrap replications estimate confidence intervals around statistics calculated data cubes.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"calculating-bootstrap-confidence-intervals","dir":"Articles","previous_headings":"","what":"Calculating bootstrap confidence intervals","title":"Calculating Bootstrap Confidence Intervals","text":"bootstrap tutorial, introduced bootstrapping way assess variability statistics calculated data cubes. Bootstrapping involves repeatedly resampling dataset recalculating statistic create distribution possible outcomes (= bootstrap replicates). tutorial builds foundation showing compute confidence intervals bootstrap replicates. Confidence intervals provide useful summary uncertainty indicating range within true value statistic likely . consider four different types intervals (confidence level \\(\\alpha\\)). choice confidence interval types calculation line boot package R (Canty & Ripley, 1999), ensure ease implementation. based definitions provided Davison & Hinkley (1997, Chapter 5) (see also DiCiccio & Efron, 1996; Efron, 1987).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"percentile","dir":"Articles","previous_headings":"Calculating bootstrap confidence intervals","what":"1. Percentile","title":"Calculating Bootstrap Confidence Intervals","text":"Uses percentiles bootstrap distribution. \\[ CI_{\\text{perc}} = \\left[ \\hat{\\theta}^*_{(\\alpha/2)}, \\hat{\\theta}^*_{(1-\\alpha/2)} \\right] \\] \\(\\hat{\\theta}^*_{(\\alpha/2)}\\) \\(\\hat{\\theta}^*_{(1-\\alpha/2)}\\) \\(\\alpha/2\\) \\(1-\\alpha/2\\) percentiles bootstrap distribution, respectively.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"bias-corrected-and-accelerated-bca","dir":"Articles","previous_headings":"Calculating bootstrap confidence intervals","what":"2. Bias-Corrected and Accelerated (BCa)","title":"Calculating Bootstrap Confidence Intervals","text":"Adjusts bias acceleration. Bias refers systematic difference observed statistic original dataset center bootstrap distribution statistic. bias correction term calculated : \\[ \\hat{z}_0 = \\Phi^{-1}\\left(\\frac{\\#(\\hat{\\theta}^*_b < \\hat{\\theta})}{B}\\right) \\] \\(\\#\\) counting operator, counting number times \\(\\hat{\\theta}^*_b\\) smaller \\(\\hat{\\theta}\\), \\(\\Phi^{-1}\\) inverse cumulative density function standard normal distribution. \\(B\\) number bootstrap samples. Acceleration quantifies sensitive variability statistic changes data. See calculated: \\(= 0\\): statistic’s variability depend data (e.g., symmetric distribution) \\(> 0\\): Small changes data large effect statistic’s variability (e.g., positive skew) \\(< 0\\): Small changes data smaller effect statistic’s variability (e.g., negative skew) bias acceleration estimates used calculate adjusted percentiles: \\[ \\alpha_1 = \\Phi\\left( \\hat{z}_0 + \\frac{\\hat{z}_0 + z_{\\alpha/2}}{1 - \\hat{}(\\hat{z}_0 + z_{\\alpha/2})} \\right), \\quad \\alpha_2 = \\Phi\\left( \\hat{z}_0 + \\frac{\\hat{z}_0 + z_{1 - \\alpha/2}}{1 - \\hat{}(\\hat{z}_0 + z_{1 - \\alpha/2})} \\right) \\] , get: \\[ CI_{\\text{bca}} = \\left[ \\hat{\\theta}^*_{(\\alpha_1)}, \\hat{\\theta}^*_{(\\alpha_2)} \\right] \\]","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"normal","dir":"Articles","previous_headings":"Calculating bootstrap confidence intervals","what":"3. Normal","title":"Calculating Bootstrap Confidence Intervals","text":"Assumes bootstrap distribution statistic approximately normal: \\[ CI_{\\text{norm}} = \\left[\\hat{\\theta} - \\text{Bias}_{\\text{boot}} - \\text{SE}_{\\text{boot}} \\cdot z_{1-\\alpha/2}, \\hat{\\theta} - \\text{Bias}_{\\text{boot}} + \\text{SE}_{\\text{boot}} \\cdot z_{1-\\alpha/2} \\right] \\] \\(z_{1-\\alpha/2}\\) \\(1-\\alpha/2\\) quantile standard normal distribution.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"basic","dir":"Articles","previous_headings":"Calculating bootstrap confidence intervals","what":"4. Basic","title":"Calculating Bootstrap Confidence Intervals","text":"Centers interval using percentiles: \\[ CI_{\\text{basic}} = \\left[ 2\\hat{\\theta} - \\hat{\\theta}^*_{(1-\\alpha/2)}, 2\\hat{\\theta} - \\hat{\\theta}^*_{(\\alpha/2)} \\right] \\] \\(\\hat{\\theta}^*_{(\\alpha/2)}\\) \\(\\hat{\\theta}^*_{(1-\\alpha/2)}\\) \\(\\alpha/2\\) \\(1-\\alpha/2\\) percentiles bootstrap distribution, respectively.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"calculating-acceleration","dir":"Articles","previous_headings":"","what":"Calculating acceleration","title":"Calculating Bootstrap Confidence Intervals","text":"acceleration calculated follows: \\[ \\hat{} = \\frac{1}{6} \\frac{\\sum_{= 1}^{n}(I_i^3)}{\\left( \\sum_{= 1}^{n}(I_i^2) \\right)^{3/2}} \\] \\(I_i\\) denotes influence data point \\(x_i\\) estimation \\(\\theta\\). \\(I_i\\) can estimated using jackknifing. Examples (1) negative jackknife: \\(I_i = (n-1)(\\hat{\\theta} - \\hat{\\theta}_{-})\\), (2) positive jackknife \\(I_i = (n+1)(\\hat{\\theta}_{-} - \\hat{\\theta})\\) (Frangos & Schucany, 1990). , \\(\\hat{\\theta}_{-}\\) estimated value leaving \\(\\)’th data point \\(x_i\\). boot package also offers infinitesimal jackknife regression estimation. Implementation jackknife algorithms can explored future. case BCa interval, calculate_bootstrap_ci() uses function calculate_acceleration() calculate acceleration. latter can also used calculate acceleration values quantify sensitivity statistic’s variability changes dataset. jackknifing, uses perform_jackknifing() function exported dubicube.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"getting-started-with-dubicube","dir":"Articles","previous_headings":"","what":"Getting started with dubicube","title":"Calculating Bootstrap Confidence Intervals","text":"method can used dataframe statistic calculated grouping variable present. tutorial, focus occurrence cubes. Therefore, use b3gbi package processing raw data go bootstrapping.","code":"# Load packages library(ggplot2)      # Data visualisation library(dplyr)        # Data wrangling library(tidyr)        # Data wrangling  # Data loading and processing library(frictionless) # Load example datasets library(b3gbi)        # Process occurrence cubes library(dubicube)     # Analysis of data quality & indicator uncertainty"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"loading-and-processing-the-data","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Loading and processing the data","title":"Calculating Bootstrap Confidence Intervals","text":"load bird cube data b3data data package using frictionless (see also ). occurrence cube birds Belgium 2000 en 2024 using MGRS grid 10 km scale. process cube b3gbi. First, select 2000 random rows make dataset smaller. reduce computation time tutorial. select data 2011 - 2020.","code":"# Read data package b3data_package <- read_package(   \"https://zenodo.org/records/15211029/files/datapackage.json\" )  # Load bird cube data bird_cube_belgium <- read_resource(b3data_package, \"bird_cube_belgium_mgrs10\") head(bird_cube_belgium) #> # A tibble: 6 × 8 #>    year mgrscode specieskey species          family     n mincoordinateuncerta…¹ #>   <dbl> <chr>         <dbl> <chr>            <chr>  <dbl>                  <dbl> #> 1  2000 31UDS65     2473958 Perdix perdix    Phasi…     1                   3536 #> 2  2000 31UDS65     2474156 Coturnix coturn… Phasi…     1                   3536 #> 3  2000 31UDS65     2474377 Fulica atra      Ralli…     5                   1000 #> 4  2000 31UDS65     2475443 Merops apiaster  Merop…     6                   1000 #> 5  2000 31UDS65     2480242 Vanellus vanell… Chara…     1                   3536 #> 6  2000 31UDS65     2480637 Accipiter nisus  Accip…     1                   3536 #> # ℹ abbreviated name: ¹​mincoordinateuncertaintyinmeters #> # ℹ 1 more variable: familycount <dbl> set.seed(123)  # Make dataset smaller rows <- sample(nrow(bird_cube_belgium), 2000) bird_cube_belgium <- bird_cube_belgium[rows, ]  # Process cube processed_cube <- process_cube(   bird_cube_belgium,   first_year = 2011,   last_year = 2020,   cols_occurrences = \"n\" ) processed_cube #>  #> Processed data cube for calculating biodiversity indicators #>  #> Date Range: 2011 - 2020  #> Single-resolution cube with cell size 10km ^2  #> Number of cells: 242  #> Grid reference system: mgrs  #> Coordinate range: #>    xmin    xmax    ymin    ymax  #>  280000  710000 5490000 5700000  #>  #> Total number of observations: 45143  #> Number of species represented: 253  #> Number of families represented: 57  #>  #> Kingdoms represented: Data not present  #>  #> First 10 rows of data (use n = to show more): #>  #> # A tibble: 957 × 13 #>     year cellCode taxonKey scientificName    family   obs minCoordinateUncerta…¹ #>    <dbl> <chr>       <dbl> <chr>             <chr>  <dbl>                  <dbl> #>  1  2011 31UFS56   5231918 Cuculus canorus   Cucul…    11                   3536 #>  2  2011 31UES28   5739317 Phoenicurus phoe… Musci…     6                   3536 #>  3  2011 31UFS64   6065824 Chroicocephalus … Larid…   143                   1000 #>  4  2011 31UFS96   2492576 Muscicapa striata Musci…     3                   3536 #>  5  2011 31UES04   5231198 Passer montanus   Passe…     1                   3536 #>  6  2011 31UES85   5229493 Garrulus glandar… Corvi…    23                    707 #>  7  2011 31UES88  10124612 Anser anser x Br… Anati…     1                    100 #>  8  2011 31UES22   2481172 Larus marinus     Larid…     8                   1000 #>  9  2011 31UFS43   2481139 Larus argentatus  Larid…    10                   3536 #> 10  2011 31UFT00   9274012 Spatula querqued… Anati…     8                   3536 #> # ℹ 947 more rows #> # ℹ abbreviated name: ¹​minCoordinateUncertaintyInMeters #> # ℹ 6 more variables: familyCount <dbl>, xcoord <dbl>, ycoord <dbl>, #> #   utmzone <int>, hemisphere <chr>, resolution <chr>"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"analysis-of-the-data","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Analysis of the data","title":"Calculating Bootstrap Confidence Intervals","text":"Let’s say interested mean number observations per grid cell per year. create function calculate . get following results: , values don’t reveal much uncertainty surrounds . better understand variability, use bootstrapping estimate distribution yearly means. distribution, can calculate bootstrap confidence intervals.","code":"# Function to calculate statistic of interest # Mean number of observations per grid cell per year mean_obs <- function(data) {   data %>%     dplyr::mutate(x = mean(obs), .by = \"cellCode\") %>%     dplyr::summarise(diversity_val = mean(x), .by = \"year\") %>%     as.data.frame() } mean_obs(processed_cube$data) #>    year diversity_val #> 1  2011      34.17777 #> 2  2012      35.27201 #> 3  2013      33.25581 #> 4  2014      55.44160 #> 5  2015      49.24754 #> 6  2016      48.34063 #> 7  2017      70.42202 #> 8  2018      48.83850 #> 9  2019      47.46795 #> 10 2020      43.00411"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"bootstrapping","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Bootstrapping","title":"Calculating Bootstrap Confidence Intervals","text":"use bootstrap_cube() function perform bootstrapping (see also bootstrap tutorial).","code":"bootstrap_results <- bootstrap_cube(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   samples = 1000,   seed = 123 ) head(bootstrap_results) #>   sample year est_original rep_boot est_boot se_boot  bias_boot #> 1      1 2011     34.17777 24.23133 33.80046 4.24489 -0.3773142 #> 2      2 2011     34.17777 24.28965 33.80046 4.24489 -0.3773142 #> 3      3 2011     34.17777 31.81445 33.80046 4.24489 -0.3773142 #> 4      4 2011     34.17777 33.42530 33.80046 4.24489 -0.3773142 #> 5      5 2011     34.17777 35.03502 33.80046 4.24489 -0.3773142 #> 6      6 2011     34.17777 33.72037 33.80046 4.24489 -0.3773142"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"interval-calculation","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Interval calculation","title":"Calculating Bootstrap Confidence Intervals","text":"Now can use calculate_bootstrap_ci() function calculate confidence limits. relies following arguments: bootstrap_samples_df: dataframe containing bootstrap replicates, row represents bootstrap sample. returned bootstrap_cube(). grouping_var: column(s) used grouping output fun(). example, fun() returns one value per year, use grouping_var = \"year\". type: character vector specifying type(s) confidence intervals compute. Options include: \"perc\": Percentile interval \"bca\": Bias-corrected accelerated interval \"norm\": Normal interval \"basic\": Basic interval \"\": Compute available interval types (default) conf: confidence level intervals. Default 0.95 (95 % confidence level). aggregate: Logical. TRUE (default), function returns confidence limits per group. FALSE, confidence limits added original bootstrap dataframe bootstrap_samples_df. data_cube: used type = \"bca\". input data processed data cube (b3gbi::process_cube()). fun: used type = \"bca\". user-defined function computes statistic(s) interest data_cube$data. function return dataframe includes column named diversity_val, containing statistic evaluate. progress: Logical flag show progress bar. Set TRUE enable progress reporting; default FALSE. get warning message BCa calculation using relatively small dataset. visualise distribution bootstrap replicates confidence intervals.  See visualising temporal trends tutorial information interval types calculated /reported temporal trends can visualised.","code":"ci_mean_obs <- calculate_bootstrap_ci(   bootstrap_samples_df = bootstrap_results,   grouping_var = \"year\",   type = c(\"perc\", \"bca\", \"norm\", \"basic\"),   conf = 0.95,   data_cube = processed_cube,   # Required for BCa   fun = mean_obs                # Required for BCa ) #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. head(ci_mean_obs) #>   year est_original est_boot   se_boot  bias_boot int_type conf       ll #> 1 2011     34.17777 33.80046  4.244890 -0.3773142     perc 0.95 26.57582 #> 2 2012     35.27201 34.45877  3.776430 -0.8132403     perc 0.95 27.23367 #> 3 2013     33.25581 33.72937  5.351881  0.4735622     perc 0.95 24.91414 #> 4 2014     55.44160 53.13004 10.597359 -2.3115608     perc 0.95 36.04170 #> 5 2015     49.24754 48.60624  9.888400 -0.6412965     perc 0.95 34.60635 #> 6 2016     48.34063 47.10668 11.388627 -1.2339492     perc 0.95 30.82583 #>         ul #> 1 42.77975 #> 2 41.66334 #> 3 46.62692 #> 4 77.37950 #> 5 73.13698 #> 6 73.16598 # Make interval type a factor ci_mean_obs <- ci_mean_obs %>%   mutate(     int_type = factor(       int_type, levels = c(\"perc\", \"bca\", \"norm\", \"basic\")     )   ) # Get bias values bias_mean_obs <- bootstrap_results %>%   distinct(year, estimate = est_original, `bootstrap estimate` = est_boot)  # Get estimate values estimate_mean_obs <- bias_mean_obs %>%   pivot_longer(cols = c(\"estimate\", \"bootstrap estimate\"),                names_to = \"Legend\", values_to = \"value\") %>%   mutate(Legend = factor(Legend, levels = c(\"estimate\", \"bootstrap estimate\"),                          ordered = TRUE)) # Visualise bootstrap_results %>%   ggplot(aes(x = year)) +   # Distribution   geom_violin(aes(y = rep_boot, group = year),               fill = alpha(\"cornflowerblue\", 0.2)) +   # Estimates and bias   geom_point(data = estimate_mean_obs, aes(y = value, shape = Legend),              colour = \"firebrick\", size = 2, alpha = 0.5) +   # Intervals   geom_errorbar(data = ci_mean_obs,                 aes(ymin = ll, ymax = ul, colour = int_type),                 position = position_dodge(0.8), linewidth = 0.8) +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\",        x = \"\", shape = \"Legend:\", colour = \"Interval type:\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results$year))) +   theme_minimal() +   theme(legend.position = \"bottom\",         legend.title = element_text(face = \"bold\"))"},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"comparison-with-a-reference-group","dir":"Articles","previous_headings":"Advanced usage of calculate_bootstrap_ci()","what":"Comparison with a reference group","title":"Calculating Bootstrap Confidence Intervals","text":"discussed bootstrap tutorial, can also compare indicator values reference group. time series analyses, often means comparing year’s indicator baseline year (e.g., first last year series). , perform bootstrapping difference indicator values. process yields bootstrap replicate distributions differences indicator values. BCa interval calculated reference group used, jackknifing implemented differently. Consider \\(\\hat{\\theta} = \\hat{\\theta}_1 - \\hat{\\theta}_2\\) \\(\\hat{\\theta}_1\\) estimate indicator value non-reference period (sample size \\(n_1\\)) \\(\\hat{\\theta}_2\\) estimate indicator value reference period (sample size \\(n_2\\)). acceleration now calculated follows: \\[ \\hat{} = \\frac{1}{6} \\frac{\\sum_{= 1}^{n_1 + n_2}(I_i^3)}{\\left( \\sum_{= 1}^{n_1 + n_2}(I_i^2) \\right)^{3/2}} \\] \\(I_i\\) can calculated using negative positive jackknife. \\(\\hat{\\theta}_{-} = \\hat{\\theta}_{1,-} - \\hat{\\theta}_2 \\text{ } = 1, \\ldots, n_1\\), \\(\\hat{\\theta}_{-} = \\hat{\\theta}_{1} - \\hat{\\theta}_{2,-} \\text{ } = n_1 + 1, \\ldots, n_1 + n_2\\) Therefore, want calculate BCa intervals using calculate_bootstrap_ci(), also need provide ref_group = 2011. see mean number observations higher years compared 2011. BCa intervals 0 2014, 2015, 2017 2018, might even say significant years. explored effect classification tutorial.  Note choice reference year well considered. Keep mind comparisons made, motivation behind reference period. high low value reference period relative periods, e.g. exceptional bad good year, can affect magnitude direction calculated differences. Whether avoided , depends motivation behind choice research question. reference period can determined legislation, start monitoring campaign. specific research question can determine periods need compared. Furthermore, variability estimate reference period affects width confidence intervals differences. variable reference period propagate greater uncertainty. case GBIF data, data available recent years earlier years. case, make sense select last period reference period. way, also avoids arbitrariness choice reference period. compare previous situations current situation (last year), repeat comparison annually, example. Finally, comparing multiple indicators, recommend using consistent reference period maintain comparability","code":"bootstrap_results_ref <- bootstrap_cube(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   samples = 1000,   ref_group = 2011,   seed = 123 ) head(bootstrap_results_ref) #>   sample year est_original   rep_boot  est_boot  se_boot  bias_boot #> 1      1 2012     1.094245  8.1881078 0.6583191 5.475053 -0.4359261 #> 2      2 2012     1.094245  7.6061946 0.6583191 5.475053 -0.4359261 #> 3      3 2012     1.094245 -4.6058908 0.6583191 5.475053 -0.4359261 #> 4      4 2012     1.094245  2.4102039 0.6583191 5.475053 -0.4359261 #> 5      5 2012     1.094245  6.2626545 0.6583191 5.475053 -0.4359261 #> 6      6 2012     1.094245 -0.1577162 0.6583191 5.475053 -0.4359261 #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. ci_mean_obs_ref %>%   filter(int_type == \"bca\") %>%   head() #>   year est_original    est_boot   se_boot  bias_boot int_type conf         ll #> 1 2012    1.0942452  0.65831911  5.475053 -0.4359261      bca 0.95 -10.762164 #> 2 2013   -0.9219589 -0.07108256  6.690590  0.8508764      bca 0.95 -12.851385 #> 3 2014   21.2638321 19.32958556 10.982093 -1.9342466      bca 0.95   5.818720 #> 4 2015   15.0697689 14.80578656 10.723767 -0.2639823      bca 0.95   2.503933 #> 5 2016   14.1628569 13.30622198 12.131657 -0.8566350      bca 0.95  -1.755266 #> 6 2017   36.2442462 43.13141123 23.943155  6.8871650      bca 0.95   7.190653 #>         ul #> 1 11.04344 #> 2 13.06589 #> 3 57.17064 #> 4 58.08685 #> 5 52.49345 #> 6 96.55713 # Make interval type factor ci_mean_obs_ref <- ci_mean_obs_ref %>%   mutate(     int_type = factor(       int_type, levels = c(\"perc\", \"bca\", \"norm\", \"basic\")     )   ) # Get bias vales bias_mean_obs <- bootstrap_results_ref %>%   distinct(year, estimate = est_original, `bootstrap estimate` = est_boot)  # Get estimate values estimate_mean_obs <- bias_mean_obs %>%   pivot_longer(cols = c(\"estimate\", \"bootstrap estimate\"),                names_to = \"Legend\", values_to = \"value\") %>%   mutate(Legend = factor(Legend, levels = c(\"estimate\", \"bootstrap estimate\"),                          ordered = TRUE)) # Visualise bootstrap_results_ref %>%   ggplot(aes(x = year)) +   # Distribution   geom_violin(aes(y = rep_boot, group = year),               fill = alpha(\"cornflowerblue\", 0.2)) +   # Estimates and bias   geom_point(data = estimate_mean_obs, aes(y = value, shape = Legend),              colour = \"firebrick\", size = 2, alpha = 0.5) +   # Intervals   geom_errorbar(data = ci_mean_obs_ref,                 aes(ymin = ll, ymax = ul, colour = int_type),                 position = position_dodge(0.8), linewidth = 0.8) +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell Compared to 2011\",        x = \"\", shape = \"Legend:\", colour = \"Interval type:\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results_ref$year))) +   theme_minimal() +   theme(legend.position = \"bottom\",         legend.title = element_text(face = \"bold\")) #> Warning: Using shapes for an ordinal variable is not advised"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"transformations","dir":"Articles","previous_headings":"Advanced usage of calculate_bootstrap_ci()","what":"Transformations","title":"Calculating Bootstrap Confidence Intervals","text":"Consider calculation Pielou’s evenness random subset data 2011-2015. take small subset dataset artificially create community high evenness. indicator values 0 1. Higher evenness values indicate balanced community (value 1 means species equally abundant), low values indicate unbalanced community (value 0 means one species dominates completely). create custom function calculate evenness: perform bootstrapping . Note can also perform bootstrapping processed_cube_even using b3gbi function pielou_evenness_ts(). calculate percentile, BCa, normal basic intervals calculate_bootstrap_ci(). get warning message BCa calculation using relatively small dataset.  notice normal basic intervals limits larger 1 impossible value evenness. intervals symmetrical around \\(\\hat{\\theta} - \\text{Bias}_{\\text{boot}}\\). can use transformation functions account . intervals calculated scale h inverse function hinv applied resulting intervals. values 0 1, can use logit function inverse: enter calculate_bootstrap_ci().  Now see intervals fall within expected range.","code":"set.seed(123)  # Make dataset smaller rows <- sample(nrow(bird_cube_belgium), 1000) bird_cube_belgium_even <- bird_cube_belgium[rows, ]  # Make dataset even bird_cube_belgium_even$n <- rnbinom(nrow(bird_cube_belgium_even),                                     size = 2, mu = 100)  # Process cube processed_cube_even <- process_cube(   bird_cube_belgium_even,   first_year = 2011,   last_year = 2015,   cols_occurrences = \"n\" ) calc_evenness <- function(data) {   data %>%     # Calculate number of observations     dplyr::group_by(year, scientificName) %>%     dplyr::summarise(obs = sum(obs), .groups = \"drop_last\") %>%     # Calculate evenness by year     dplyr::mutate(       tot = sum(obs),       p = obs / tot,       p_ln_p = p * log(p),       ln_S = log(dplyr::n_distinct(scientificName)),       diversity_val = (-sum(p_ln_p)) / ln_S     ) %>%     dplyr::ungroup() %>%     # Get distinct values     dplyr::distinct(year, diversity_val) } bootstrap_results_evenness <- bootstrap_cube(   data_cube = processed_cube_even,   fun = calc_evenness,   grouping_var = \"year\",   samples = 1000,   seed = 123 ) ci_evenness <- calculate_bootstrap_ci(   bootstrap_samples_df = bootstrap_results_evenness,   grouping_var = \"year\",   type = c(\"perc\", \"bca\", \"norm\", \"basic\"),   data_cube = processed_cube_even,   fun = calc_evenness ) #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. # Make interval type factor ci_evenness <- ci_evenness %>%   mutate(     int_type = factor(       int_type, levels = c(\"perc\", \"bca\", \"norm\", \"basic\")     )   ) # Get bias vales bias_mean_obs <- bootstrap_results_evenness %>%   distinct(year, estimate = est_original, `bootstrap estimate` = est_boot)  # Get estimate values estimate_mean_obs <- bias_mean_obs %>%   pivot_longer(cols = c(\"estimate\", \"bootstrap estimate\"),                names_to = \"Legend\", values_to = \"value\") %>%   mutate(Legend = factor(Legend, levels = c(\"estimate\", \"bootstrap estimate\"),                          ordered = TRUE)) # Visualise bootstrap_results_evenness %>%   ggplot(aes(x = year)) +   # Distribution   geom_violin(aes(y = rep_boot, group = year),               fill = alpha(\"cornflowerblue\", 0.2)) +   # Estimates and bias   geom_point(data = estimate_mean_obs, aes(y = value, shape = Legend),              colour = \"firebrick\", size = 2, alpha = 0.5) +   # Intervals   geom_errorbar(data = ci_evenness,                 aes(ymin = ll, ymax = ul, colour = int_type),                 position = position_dodge(0.8), linewidth = 0.8) +   # Settings   labs(y = \"Evenness\", x = \"\", shape = \"Legend:\", colour = \"Interval type:\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results_evenness$year))) +   theme_minimal() +   theme(legend.position = \"bottom\",         legend.title = element_text(face = \"bold\")) #> Warning: Using shapes for an ordinal variable is not advised # Logit transformation logit <- function(p) {   log(p / (1 - p)) }  # Inverse logit transformation inv_logit <- function(l) {   exp(l) / (1 + exp(l)) } ci_evenness_trans <- calculate_bootstrap_ci(   bootstrap_samples_df = bootstrap_results_evenness,   grouping_var = \"year\",   type = c(\"perc\", \"bca\", \"norm\", \"basic\"),   h = logit,   hinv = inv_logit,   data_cube = processed_cube_even,   fun = calc_evenness ) #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. # Make interval type factor ci_evenness_trans <- ci_evenness_trans %>%   mutate(     int_type = factor(       int_type, levels = c(\"perc\", \"bca\", \"norm\", \"basic\")     )   ) # Get bias vales bias_mean_obs <- bootstrap_results_evenness %>%   distinct(year, estimate = est_original, `bootstrap estimate` = est_boot)  # Get estimate values estimate_mean_obs <- bias_mean_obs %>%   pivot_longer(cols = c(\"estimate\", \"bootstrap estimate\"),                names_to = \"Legend\", values_to = \"value\") %>%   mutate(Legend = factor(Legend, levels = c(\"estimate\", \"bootstrap estimate\"),                          ordered = TRUE)) # Visualise bootstrap_results_evenness %>%   ggplot(aes(x = year)) +   # Distribution   geom_violin(aes(y = rep_boot, group = year),               fill = alpha(\"cornflowerblue\", 0.2)) +   # Estimates and bias   geom_point(data = estimate_mean_obs, aes(y = value, shape = Legend),              colour = \"firebrick\", size = 2, alpha = 0.5) +   # Intervals   geom_errorbar(data = ci_evenness_trans,                 aes(ymin = ll, ymax = ul, colour = int_type),                 position = position_dodge(0.8), linewidth = 0.8) +   # Settings   labs(y = \"Evenness\", x = \"\", shape = \"Legend:\", colour = \"Interval type:\") +   scale_y_continuous(limits = c(NA, 1)) +   scale_x_continuous(breaks = sort(unique(bootstrap_results_evenness$year))) +   theme_minimal() +   theme(legend.position = \"bottom\",         legend.title = element_text(face = \"bold\")) #> Warning: Using shapes for an ordinal variable is not advised"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"issues-with-bias-correction-for-species-richness-indicators","dir":"Articles","previous_headings":"Advanced usage of calculate_bootstrap_ci()","what":"Issues with bias correction for species richness indicators","title":"Calculating Bootstrap Confidence Intervals","text":"Consider calculation observed species richness subset used previous subsection. create custom function calculate richness: perform bootstrapping . Note can also perform bootstrapping processed_cube_small using b3gbi function obs_richness_ts(). case, group-specific bootstrap appropriate, sake simplicity stick whole-cube bootstrap . detailed discussion use approach, see tutorial. calculate percentile, BCa, normal basic intervals calculate_bootstrap_ci(). get warning message BCa calculation. bias infinite BCa intervals calculated. notice none intervals cover estimate. percentile interval account bias, BCa interval calculated bias large normal basic intervals overcompensated large bootstrap bias.  issue arises bootstrap resampling introduce new species present original sample (Dixon, 2001, p. 287). result, observed species richness — simply count unique species — tends negatively biased bootstrap replicates. leads extreme mismatch original estimate distribution bootstrap replicates. cases, BCa intervals may fail altogether (e.g., due infinite bias correction factors), bootstrap intervals (normal, basic) may overcorrect. option within calculate_bootstrap_ci() center confidence limits around original estimate (no_bias = TRUE). means bootstrap distribution used calculate confidence intervals, except bootstrap bias. may “solve” technical problems interval calculation (like infinite undefined corrections), cost ignoring bootstrap bias. approach used caution clear justification, bootstrap bias known artifact sampling limitation underlying data structure. inherent limitation, alternative richness estimators account undetected species preferred uncertainty quantification needed. vegan (Oksanen et al., 2024) iNEXT (Hsieh et al., 2016) R packages provide estimators, including Chao, Jackknife, coverage-based rarefaction/extrapolation, designed handle unseen species provide meaningful uncertainty estimates. estimators also implemented directly occurrence cubes recent versions b3gbi (≥ v0.4.0), offering integration existing cube-based workflows. However, important note alternative estimators — equivalent observed richness yield different values design. Indeed, intervals now centered around original estimate.","code":"calc_richness <- function(data) {   data %>%     dplyr::group_by(year) %>%     dplyr::summarise(diversity_val = n_distinct(scientificName),                      .groups = \"drop\") } bootstrap_results_richness <- bootstrap_cube(   data_cube = processed_cube_even,   fun = calc_richness,   grouping_var = \"year\",   samples = 1000,   seed = 123 ) ci_richness <- calculate_bootstrap_ci(   bootstrap_samples_df = bootstrap_results_richness,   grouping_var = \"year\",   type = c(\"perc\", \"bca\", \"norm\", \"basic\"),   data_cube = processed_cube_even,   fun = calc_richness ) #> Warning in bca_ci(t0 = unique(df$est_original), t = df$rep_boot, a = a, : #> Estimated adjustment 'z0' is infinite. #> Warning in bca_ci(t0 = unique(df$est_original), t = df$rep_boot, a = a, : #> Estimated adjustment 'z0' is infinite. #> Warning in bca_ci(t0 = unique(df$est_original), t = df$rep_boot, a = a, : #> Estimated adjustment 'z0' is infinite. #> Warning in bca_ci(t0 = unique(df$est_original), t = df$rep_boot, a = a, : #> Estimated adjustment 'z0' is infinite. #> Warning in bca_ci(t0 = unique(df$est_original), t = df$rep_boot, a = a, : #> Estimated adjustment 'z0' is infinite. # Make interval type factor ci_richness <- ci_richness %>%   mutate(     int_type = factor(       int_type, levels = c(\"perc\", \"bca\", \"norm\", \"basic\")     )   ) # Get bias vales bias_mean_obs <- bootstrap_results_richness %>%   distinct(year, estimate = est_original, `bootstrap estimate` = est_boot)  # Get estimate values estimate_mean_obs <- bias_mean_obs %>%   pivot_longer(cols = c(\"estimate\", \"bootstrap estimate\"),                names_to = \"Legend\", values_to = \"value\") %>%   mutate(Legend = factor(Legend, levels = c(\"estimate\", \"bootstrap estimate\"),                          ordered = TRUE)) # Visualise bootstrap_results_richness %>%   ggplot(aes(x = year)) +   # Distribution   geom_violin(aes(y = rep_boot, group = year),               fill = alpha(\"cornflowerblue\", 0.2)) +   # Estimates and bias   geom_point(data = estimate_mean_obs, aes(y = value, shape = Legend),              colour = \"firebrick\", size = 2, alpha = 0.5) +   # Intervals   geom_errorbar(data = ci_richness,                 aes(ymin = ll, ymax = ul, colour = int_type),                 position = position_dodge(0.8), linewidth = 0.8) +   # Settings   labs(y = \"Observed species richness\", x = \"\", shape = \"Legend:\",        colour = \"Interval type:\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results_richness$year))) +   theme_minimal() +   theme(legend.position = \"bottom\",         legend.title = element_text(face = \"bold\")) #> Warning: Using shapes for an ordinal variable is not advised ci_richness_no_bias <- calculate_bootstrap_ci(   bootstrap_samples_df = bootstrap_results_richness,   grouping_var = \"year\",   type = c(\"perc\", \"bca\", \"norm\", \"basic\"),   no_bias = TRUE,   data_cube = processed_cube_even,   fun = calc_richness ) # Make interval type factor ci_richness_no_bias <- ci_richness_no_bias %>%   mutate(     int_type = factor(       int_type, levels = c(\"perc\", \"bca\", \"norm\", \"basic\")     )   ) # Get bias vales bias_mean_obs <- bootstrap_results_richness %>%   distinct(year, estimate = est_original, `bootstrap estimate` = est_boot)  # Get estimate values estimate_mean_obs <- bias_mean_obs %>%   pivot_longer(cols = c(\"estimate\", \"bootstrap estimate\"),                names_to = \"Legend\", values_to = \"value\") %>%   mutate(Legend = factor(Legend, levels = c(\"estimate\", \"bootstrap estimate\"),                          ordered = TRUE)) # Visualise bootstrap_results_richness %>%   ggplot(aes(x = year)) +   # Distribution   geom_violin(aes(y = rep_boot, group = year),               fill = alpha(\"cornflowerblue\", 0.2)) +   # Estimates and bias   geom_point(data = estimate_mean_obs, aes(y = value, shape = Legend),              colour = \"firebrick\", size = 2, alpha = 0.5) +   # Intervals   geom_errorbar(data = ci_richness_no_bias,                 aes(ymin = ll, ymax = ul, colour = int_type),                 position = position_dodge(0.8), linewidth = 0.8) +   # Settings   labs(y = \"Observed species richness\", x = \"\", shape = \"Legend:\",        colour = \"Interval type:\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results_richness$year))) +   theme_minimal() +   theme(legend.position = \"bottom\",         legend.title = element_text(face = \"bold\")) #> Warning: Using shapes for an ordinal variable is not advised"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-interval-calculation.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Calculating Bootstrap Confidence Intervals","text":"Canty, ., & Ripley, B. (1999). boot: Bootstrap Functions (Originally Angelo Canty S) [Computer software]. https://CRAN.R-project.org/package=boot Davison, . C., & Hinkley, D. V. (1997). Bootstrap Methods Application (1st ed.). Cambridge University Press. https://doi.org/10.1017/CBO9780511802843 DiCiccio, T. J., & Efron, B. (1996). Bootstrap confidence intervals. Statistical Science, 11(3). https://doi.org/10.1214/ss/1032280214 Dixon, P. M. (2001). Bootstrap Jackknife: Describing Precision Ecological Indices. S. M. Scheiner & J. Gurevitch (Eds.), Design Analysis Ecological Experiments (Second Edition, pp. 267–288). Oxford University PressNew York, NY. https://doi.org/10.1093/oso/9780195131871.003.0014 Efron, B. (1987). Better Bootstrap Confidence Intervals. Journal American Statistical Association, 82(397), 171–185. https://doi.org/10.1080/01621459.1987.10478410 Frangos, C. C., & Schucany, W. R. (1990). Jackknife estimation bootstrap acceleration constant. Computational Statistics & Data Analysis, 9(3), 271–281. https://doi.org/10.1016/0167-9473(90)90109-U","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Bootstrap Method for Data Cubes","text":"working data cubes, ’s essential understand uncertainty surrounding derived statistics. tutorial introduces bootstrap_cube() function dubicube, uses bootstrap resampling estimate variability, bias, standard error estimates.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"bootstrapping-for-data-cubes","dir":"Articles","previous_headings":"","what":"Bootstrapping for data cubes","title":"Bootstrap Method for Data Cubes","text":"Bootstrapping resampling method used approximate distribution statistic repeatedly sampling data replacement. context biodiversity data cubes, bootstrap_cube() enables us assess variability derived statistics, computing confidence intervals. Note also make distinction whole-cube bootstrapping group-specific bootstrapping. explained tutorial.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"a-data-cube-and-a-statistic","dir":"Articles","previous_headings":"Bootstrapping for data cubes","what":"A data cube and a statistic","title":"Bootstrap Method for Data Cubes","text":"Consider data cube \\(\\mathbf{X}\\) want calculate statistic \\(\\theta\\). initial set data points. , \\(n\\) sample size. corresponds number cells data cube number rows tabular format. parameter statistic estimated, mean \\(\\bar{X}\\), variance \\(\\sigma^2\\), biodiversity indicator. Let \\(\\hat{\\theta}\\) denote estimated value \\(\\theta\\) calculated complete dataset \\(\\mathbf{X}\\).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"resampling-and-recalculating","dir":"Articles","previous_headings":"Bootstrapping for data cubes","what":"Resampling and recalculating","title":"Bootstrap Method for Data Cubes","text":"\\(\\mathbf{X}\\), multiple resampled datasets \\(\\mathbf{X}^*\\) created sampling rows (cells) replacement new dataset equally large \\(\\mathbf{X}\\). process repeated \\(B\\) times, resample yields new estimate \\(\\hat{\\theta}^*_b\\) statistic. sample size \\(n\\) drawn replacement original sample \\(\\mathbf{X}\\). \\(X_i^*\\) drawn independently \\(\\mathbf{X}\\). total \\(B\\) bootstrap samples drawn original data. Common choices \\(B\\) 1000 10,000 ensure good approximation distribution bootstrap replications (see ). value statistic interest calculated \\(b\\)-th bootstrap sample \\(\\mathbf{X}^*_b\\). example, \\(\\theta\\) sample mean, \\(\\hat{\\theta}^*_b = \\bar{X}^*_b\\).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"derivation-of-bootstrap-statistics","dir":"Articles","previous_headings":"Bootstrapping for data cubes","what":"Derivation of bootstrap statistics","title":"Bootstrap Method for Data Cubes","text":"set bootstrap replications forms bootstrap distribution, can used estimate bootstrap statistics construct confidence intervals (see interval calculation tutorial). average bootstrap replications: \\[ \\hat{\\theta}_{\\text{boot}} = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*_b \\] bias indicates much bootstrap estimate deviates original sample estimate. calculated difference average bootstrap estimate original estimate: \\[ \\text{Bias}_{\\text{boot}} = \\frac{1}{B} \\sum_{b=1}^B (\\hat{\\theta}^*_b - \\hat{\\theta}) = \\hat{\\theta}_{\\text{boot}} - \\hat{\\theta} \\] standard deviation bootstrap replications, estimates variability statistic.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"getting-started-with-dubicube","dir":"Articles","previous_headings":"","what":"Getting started with dubicube","title":"Bootstrap Method for Data Cubes","text":"dubicube bootstrapping method can used dataframe statistic calculated grouping variable present. tutorial, focus occurrence cubes. Therefore, use b3gbi package processing raw data moving bootstrapping.","code":"# Load packages library(ggplot2)      # Data visualisation library(dplyr)        # Data wrangling library(tidyr)        # Data wrangling  # Data loading and processing library(frictionless) # Load example datasets library(b3gbi)        # Process occurrence cubes library(dubicube)     # Analysis of data quality & indicator uncertainty"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"loading-and-processing-the-data","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Loading and processing the data","title":"Bootstrap Method for Data Cubes","text":"load bird cube data b3data data package using frictionless (see also ). occurrence cube birds Belgium 2000 en 2024 using MGRS grid 10 km scale. process cube b3gbi. First, select 2000 random rows make dataset smaller. reduce computation time tutorial. select data 2011 - 2020.","code":"# Read data package b3data_package <- read_package(   \"https://zenodo.org/records/15211029/files/datapackage.json\" )  # Load bird cube data bird_cube_belgium <- read_resource(b3data_package, \"bird_cube_belgium_mgrs10\") head(bird_cube_belgium) #> # A tibble: 6 × 8 #>    year mgrscode specieskey species          family     n mincoordinateuncerta…¹ #>   <dbl> <chr>         <dbl> <chr>            <chr>  <dbl>                  <dbl> #> 1  2000 31UDS65     2473958 Perdix perdix    Phasi…     1                   3536 #> 2  2000 31UDS65     2474156 Coturnix coturn… Phasi…     1                   3536 #> 3  2000 31UDS65     2474377 Fulica atra      Ralli…     5                   1000 #> 4  2000 31UDS65     2475443 Merops apiaster  Merop…     6                   1000 #> 5  2000 31UDS65     2480242 Vanellus vanell… Chara…     1                   3536 #> 6  2000 31UDS65     2480637 Accipiter nisus  Accip…     1                   3536 #> # ℹ abbreviated name: ¹​mincoordinateuncertaintyinmeters #> # ℹ 1 more variable: familycount <dbl> set.seed(123)  # Make dataset smaller rows <- sample(nrow(bird_cube_belgium), 2000) bird_cube_belgium <- bird_cube_belgium[rows, ]  # Process cube processed_cube <- process_cube(   bird_cube_belgium,   first_year = 2011,   last_year = 2020,   cols_occurrences = \"n\" ) processed_cube #>  #> Processed data cube for calculating biodiversity indicators #>  #> Date Range: 2011 - 2020  #> Single-resolution cube with cell size 10km ^2  #> Number of cells: 242  #> Grid reference system: mgrs  #> Coordinate range: #>    xmin    xmax    ymin    ymax  #>  280000  710000 5490000 5700000  #>  #> Total number of observations: 45143  #> Number of species represented: 253  #> Number of families represented: 57  #>  #> Kingdoms represented: Data not present  #>  #> First 10 rows of data (use n = to show more): #>  #> # A tibble: 957 × 13 #>     year cellCode taxonKey scientificName    family   obs minCoordinateUncerta…¹ #>    <dbl> <chr>       <dbl> <chr>             <chr>  <dbl>                  <dbl> #>  1  2011 31UFS56   5231918 Cuculus canorus   Cucul…    11                   3536 #>  2  2011 31UES28   5739317 Phoenicurus phoe… Musci…     6                   3536 #>  3  2011 31UFS64   6065824 Chroicocephalus … Larid…   143                   1000 #>  4  2011 31UFS96   2492576 Muscicapa striata Musci…     3                   3536 #>  5  2011 31UES04   5231198 Passer montanus   Passe…     1                   3536 #>  6  2011 31UES85   5229493 Garrulus glandar… Corvi…    23                    707 #>  7  2011 31UES88  10124612 Anser anser x Br… Anati…     1                    100 #>  8  2011 31UES22   2481172 Larus marinus     Larid…     8                   1000 #>  9  2011 31UFS43   2481139 Larus argentatus  Larid…    10                   3536 #> 10  2011 31UFT00   9274012 Spatula querqued… Anati…     8                   3536 #> # ℹ 947 more rows #> # ℹ abbreviated name: ¹​minCoordinateUncertaintyInMeters #> # ℹ 6 more variables: familyCount <dbl>, xcoord <dbl>, ycoord <dbl>, #> #   utmzone <int>, hemisphere <chr>, resolution <chr>"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"analysis-of-the-data","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Analysis of the data","title":"Bootstrap Method for Data Cubes","text":"Let’s say interested mean number observations per grid cell per year. create function calculate . get following results: , values don’t reveal much uncertainty surrounds . better understand variability, use bootstrapping estimate distribution yearly means.","code":"# Function to calculate the statistic of interest # Mean observations per grid cell per year mean_obs <- function(data) {   obs <- x <- NULL    data %>%     dplyr::mutate(x = mean(obs), .by = \"cellCode\") %>%     dplyr::summarise(diversity_val = mean(x), .by = \"year\") %>%     as.data.frame() } mean_obs(processed_cube$data) #>    year diversity_val #> 1  2011      34.17777 #> 2  2012      35.27201 #> 3  2013      33.25581 #> 4  2014      55.44160 #> 5  2015      49.24754 #> 6  2016      48.34063 #> 7  2017      70.42202 #> 8  2018      48.83850 #> 9  2019      47.46795 #> 10 2020      43.00411"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"bootstrapping","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Bootstrapping","title":"Bootstrap Method for Data Cubes","text":"use bootstrap_cube() function . relies following arguments: data_cube: input data processed data cube (b3gbi::process_cube()). fun: user-defined function computes statistic(s) interest data_cube$data. function return dataframe includes column named diversity_val, containing statistic evaluate. grouping_var: column(s) used grouping output fun(). example, fun() returns one value per year, use grouping_var = \"year\". samples: number bootstrap samples draw. Common values 1000 reliable estimates variability confidence intervals. seed: optional numeric seed ensure reproducibility bootstrap resampling. Set fixed value get consistent results across runs. progress: Logical flag show progress bar. Set TRUE enable progress reporting; default FALSE. can visualise bootstrap distributions using violin plot.","code":"bootstrap_results <- bootstrap_cube(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   samples = 1000,   seed = 123 ) head(bootstrap_results) #>   sample year est_original rep_boot est_boot se_boot  bias_boot #> 1      1 2011     34.17777 24.23133 33.80046 4.24489 -0.3773142 #> 2      2 2011     34.17777 24.28965 33.80046 4.24489 -0.3773142 #> 3      3 2011     34.17777 31.81445 33.80046 4.24489 -0.3773142 #> 4      4 2011     34.17777 33.42530 33.80046 4.24489 -0.3773142 #> 5      5 2011     34.17777 35.03502 33.80046 4.24489 -0.3773142 #> 6      6 2011     34.17777 33.72037 33.80046 4.24489 -0.3773142 # Get bias vales bias_mean_obs <- bootstrap_results %>%   distinct(year, estimate = est_original, `bootstrap estimate` = est_boot)  # Get estimate values estimate_mean_obs <- bias_mean_obs %>%   pivot_longer(cols = c(\"estimate\", \"bootstrap estimate\"),                names_to = \"Legend\", values_to = \"value\") %>%   mutate(Legend = factor(Legend, levels = c(\"estimate\", \"bootstrap estimate\"),                          ordered = TRUE))  # Visualise bootrap distributions bootstrap_results %>%   ggplot(aes(x = year)) +   # Distribution   geom_violin(aes(y = rep_boot, group = year),               fill = alpha(\"cornflowerblue\", 0.2)) +   # Estimates and bias   geom_point(data = estimate_mean_obs, aes(y = value, shape = Legend),              colour = \"firebrick\", size = 2.5) +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\",        x = \"\", shape = \"Legend:\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results$year))) +   theme_minimal() +   theme(legend.position = \"bottom\",         legend.title = element_text(face = \"bold\"))"},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"bootstrap-simple-dataframes","dir":"Articles","previous_headings":"Advanced usage of bootstrap_cube()","what":"Bootstrap simple dataframes","title":"Bootstrap Method for Data Cubes","text":"stated documentation, also possible bootstrap dataframe. case, set argument processed_cube = FALSE. implemented allow flexible use simple dataframes, still encouraging use b3gbi::process_cube() default functionality.","code":"bootstrap_results_df <- bootstrap_cube(   data_cube = processed_cube$data,      # data.frame object   fun = mean_obs,   grouping_var = \"year\",   samples = 1000,   seed = 123,   processed_cube = FALSE )"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/bootstrap-method-cubes.html","id":"comparison-with-a-reference-group","dir":"Articles","previous_headings":"Advanced usage of bootstrap_cube()","what":"Comparison with a reference group","title":"Bootstrap Method for Data Cubes","text":"particularly insightful approach comparing indicator values reference group. time series analyses, often means comparing year’s indicator baseline year (e.g., first last year series). , perform bootstrapping differences indicator values: Resample dataset replacement Calculate indicator group (e.g., year) non-reference group, compute difference indicator value reference group Repeat steps 1–3 across bootstrap iterations process yields bootstrap replicate distributions differences indicator values. see mean number observations higher years compared 2011. point can say differences significant? explored effect classification tutorial.  Note choice reference year well considered. Keep mind comparisons made, motivation behind reference period. high low value reference period relative periods, e.g. exceptional bad good year, can affect magnitude direction calculated differences. Whether avoided , depends motivation behind choice research question. reference period can determined legislation, start monitoring campaign. specific research question can determine periods need compared. Furthermore, variability estimate reference period affects width confidence intervals differences. variable reference period propagate greater uncertainty. case GBIF data, data available recent years earlier years. case, make sense select last period reference period. way, also avoids arbitrariness choosing reference period. compare previous situations current situation (last year), repeat comparison annually, example. Finally, comparing multiple indicators, recommend using consistent reference period maintain comparability.","code":"bootstrap_results_ref <- bootstrap_cube(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   samples = 1000,   ref_group = 2011,   seed = 123 ) head(bootstrap_results_ref) #>   sample year est_original   rep_boot  est_boot  se_boot  bias_boot #> 1      1 2012     1.094245  8.1881078 0.6583191 5.475053 -0.4359261 #> 2      2 2012     1.094245  7.6061946 0.6583191 5.475053 -0.4359261 #> 3      3 2012     1.094245 -4.6058908 0.6583191 5.475053 -0.4359261 #> 4      4 2012     1.094245  2.4102039 0.6583191 5.475053 -0.4359261 #> 5      5 2012     1.094245  6.2626545 0.6583191 5.475053 -0.4359261 #> 6      6 2012     1.094245 -0.1577162 0.6583191 5.475053 -0.4359261 # Get bias vales bias_mean_obs <- bootstrap_results_ref %>%   distinct(year, estimate = est_original, `bootstrap estimate` = est_boot)  # Get estimate values estimate_mean_obs <- bias_mean_obs %>%   pivot_longer(cols = c(\"estimate\", \"bootstrap estimate\"),                names_to = \"Legend\", values_to = \"value\") %>%   mutate(Legend = factor(Legend, levels = c(\"estimate\", \"bootstrap estimate\"),                          ordered = TRUE))  # Visualise bootrap distributions bootstrap_results_ref %>%   ggplot(aes(x = year)) +   # Distribution   geom_violin(aes(y = rep_boot, group = year),               fill = alpha(\"cornflowerblue\", 0.2)) +   # Estimates and bias   geom_point(data = estimate_mean_obs, aes(y = value, shape = Legend),              colour = \"firebrick\", size = 2) +   # Settings   labs(y = \"Mean Number of Observations per Grid Cell\\nCompared to 2011\",        x = \"\", shape = \"Legend:\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results_ref$year))) +   theme_minimal() +   theme(legend.position = \"bottom\",         legend.title = element_text(face = \"bold\"))"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/effect-classification.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Classifying Effects Using Confidence Limits","text":"tutorial demonstrates use add_effect_classification() function classify effect sizes based confidence intervals. useful want interpret uncertainty trends treatment effects using transparent rule-based approach.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/effect-classification.html","id":"about-effect-classification","dir":"Articles","previous_headings":"","what":"About effect classification","title":"Classifying Effects Using Confidence Limits","text":"add_effect_classification() function takes dataframe confidence limits adds classification columns describe: direction strength effect (e.g., strong increase, moderate decrease, unknown) fine-grained coarse interpretations. uses effectclass package internally perform classification logic. Classification based : lower upper confidence limits around effect estimate reference value (e.g., 0 effect) threshold range thresholds define counts meaningful effect Thresholds reference values reflect scientifically practically meaningful changes — e.g., biologically relevant increase decline biodiversity indicator. See effectclass tutorial elaborate overview.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/effect-classification.html","id":"getting-started-with-dubicube","dir":"Articles","previous_headings":"","what":"Getting started with dubicube","title":"Classifying Effects Using Confidence Limits","text":"dataframe must contain lower upper confidence limits, e.g. \"lcl\" \"ucl\". Let’s start synthetic dataset 10 observations.","code":"# Load packages library(ggplot2)      # Data visualisation library(dubicube)     # Analysis of data quality & indicator uncertainty # Simulated means and standard deviations ds <- data.frame(   mean = c(0, 0.5, -0.5, 1, -1, 1.5, -1.5, 0.5, -0.5, 0),   sd = c(1, 0.5, 0.5, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25, 0.5) )  # Compute 90% confidence intervals ds$lcl <- qnorm(0.05, ds$mean, ds$sd) ds$ucl <- qnorm(0.95, ds$mean, ds$sd)  # View the dataset ds #>    mean   sd         lcl         ucl #> 1   0.0 1.00 -1.64485363  1.64485363 #> 2   0.5 0.50 -0.32242681  1.32242681 #> 3  -0.5 0.50 -1.32242681  0.32242681 #> 4   1.0 0.50  0.17757319  1.82242681 #> 5  -1.0 0.50 -1.82242681 -0.17757319 #> 6   1.5 0.25  1.08878659  1.91121341 #> 7  -1.5 0.25 -1.91121341 -1.08878659 #> 8   0.5 0.25  0.08878659  0.91121341 #> 9  -0.5 0.25 -0.91121341 -0.08878659 #> 10  0.0 0.50 -0.82242681  0.82242681"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/effect-classification.html","id":"add-effect-classifications","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Add effect classifications","title":"Classifying Effects Using Confidence Limits","text":"use add_effect_classification() effect classification. relies following arguments: df: input data, dataframe containing confidence limits, e.g. result bootstrap confidence interval calculation. cl_columns: character vector length 2 specifying column names df contain lower upper confidence limits. example: cl_columns = c(\"lcl\", \"ucl\"). threshold: numeric vector defining effect size threshold(s) used distinguish “effect” meaningful increases decreases. can either one number (interpreted symmetrically around reference) two (explicit lower upper thresholds). reference: single numeric value representing null hypothesis -effect level (e.g. 0). position confidence interval relative value determines direction effect. coarse: logical flag indicating whether simplified classification also added. TRUE (default), additional columns effect_code_coarse effect_coarse included, summarizing effects broader categories (increase, decrease, stable, unknown). Let’s classify effects using threshold 1 (expanded [-1, 1] around reference) reference 0.","code":"# Perform effect classification result <- add_effect_classification(   df = ds,   cl_columns = c(\"lcl\", \"ucl\"),   threshold = 1,   reference = 0,   coarse = TRUE )  # View the result result #>    mean   sd         lcl         ucl effect_code effect_code_coarse #> 1   0.0 1.00 -1.64485363  1.64485363           ?                  ? #> 2   0.5 0.50 -0.32242681  1.32242681          ?+                  ? #> 3  -0.5 0.50 -1.32242681  0.32242681          ?-                  ? #> 4   1.0 0.50  0.17757319  1.82242681           +                  + #> 5  -1.0 0.50 -1.82242681 -0.17757319           -                  - #> 6   1.5 0.25  1.08878659  1.91121341          ++                  + #> 7  -1.5 0.25 -1.91121341 -1.08878659          --                  - #> 8   0.5 0.25  0.08878659  0.91121341          +~                  + #> 9  -0.5 0.25 -0.91121341 -0.08878659          -~                  - #> 10  0.0 0.50 -0.82242681  0.82242681           ~                  ~ #>                effect effect_coarse #> 1             unknown       unknown #> 2  potential increase       unknown #> 3  potential decrease       unknown #> 4            increase      increase #> 5            decrease      decrease #> 6     strong increase      increase #> 7     strong decrease      decrease #> 8   moderate increase      increase #> 9   moderate decrease      decrease #> 10             stable        stable"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/effect-classification.html","id":"visualising-the-result","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Visualising the result","title":"Classifying Effects Using Confidence Limits","text":"Detailed guidance best practices visualising effect classifications provided tutorials temporal trends spatial trends. , demonstrate simple direct way visualise classified effects using ggplot2. following plot shows point estimates 90% confidence intervals, coloured coarse effect classification.  plot provides detailed view using fine-grained classification.","code":"# Define coarse colour palette coarse_colour <- scale_colour_manual(   values =  c(     \"chartreuse3\",     \"gold\",     \"firebrick1\",     \"skyblue\"   ),   drop = FALSE )  # Define fine colour palette fine_colour <- scale_colour_manual(   values =  c(     \"darkgreen\",     \"chartreuse3\",     \"darkolivegreen1\",     \"gold\",     \"orange\",     \"firebrick1\",     \"darkred\",     \"gray80\",     \"gray30\",     \"grey55\"   ),   drop = FALSE ) ggplot(data = result, aes(x = as.numeric(rownames(result)))) +   geom_hline(yintercept = 0, linetype = \"longdash\", colour = \"black\") +   geom_hline(yintercept = c(-1, 1), linetype = \"dotdash\") +   geom_errorbar(aes(ymin = lcl, ymax = ucl, colour = effect_coarse),                 linewidth = 1.5, show.legend = TRUE) +   geom_point(aes(y = mean), colour = \"black\", size = 3.5) +   labs(x = \"Observation\", y = \"Effect estimate\", colour = \"Effect (coarse)\") +   coarse_colour +   theme_minimal() ggplot(data = result, aes(x = as.numeric(rownames(result)))) +   geom_hline(yintercept = 0, linetype = \"longdash\", colour = \"black\") +   geom_hline(yintercept = c(-1, 1), linetype = \"dotdash\") +   geom_errorbar(aes(ymin = lcl, ymax = ucl, colour = effect),                 linewidth = 1.5, show.legend = TRUE) +   geom_point(aes(y = mean), colour = \"black\", size = 3.5) +   labs(x = \"Observation\", y = \"Effect estimate\", colour = \"Effect (fine)\") +   fine_colour +   theme_minimal()"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Group-Level Sensitivity Analysis","text":"working biodiversity data cubes, understanding sensitive indicators individual groups (like species, sites, time periods) can crucial robust analysis. tutorial introduces cross_validate_cube() function dubicube, helps assess individual categories influence biodiversity indicators using cross-validation.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"group-level-cross-validation-for-data-cubes","dir":"Articles","previous_headings":"","what":"Group-level cross-validation for data cubes","title":"Group-Level Sensitivity Analysis","text":"Cross-validation resampling technique commonly used statistics machine learning evaluate model performance. context biodiversity data cubes, adapt cross-validation assess robust indicators inclusion exclusion specific groups. helps identify influential categories disproportionately affect indicator values.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"a-data-cube-and-a-statistic","dir":"Articles","previous_headings":"Group-level cross-validation for data cubes","what":"A data cube and a statistic","title":"Group-Level Sensitivity Analysis","text":"Consider data cube \\(\\mathbf{X}\\) want calculate statistic \\(\\theta\\). data cube can grouped, e.g. taxon, contains multiple categories, e.g. species1, species2, species3 … initial set data points, \\(s\\) different categories group (e.g. \\(s = 10\\) species group taxon) \\(n\\) total samples across categories (= sample size). \\(n\\) corresponds number cells data cube number rows tabular format. parameter statistic estimated, mean \\(\\bar{X}\\), variance \\(\\sigma^2\\), biodiversity indicator. Let \\(\\hat{\\theta}\\) denote estimated value \\(\\theta\\) calculated complete dataset \\(\\mathbf{X}\\).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"resampling-and-recalculating","dir":"Articles","previous_headings":"Group-level cross-validation for data cubes","what":"Resampling and recalculating","title":"Group-Level Sensitivity Analysis","text":"\\(\\mathbf{X}\\), multiple data cubes \\(\\mathbf{X}_{-s_j}\\) created new dataset different category removed compared original data cube \\(\\mathbf{X}\\). example dataset 1, species1 excluded; dataset 2 species2; dataset 3 species3; . new dataset, statistic \\(\\theta\\) calculated: \\(\\hat{\\theta}_{-s_j}\\). full dataset \\(\\mathbf{X}\\) excluding samples belonging category \\(j\\). subset used investigate influence category \\(j\\) estimated statistic \\(\\hat{\\theta}\\). value statistic interest calculated \\(\\mathbf{X}_{-s_j}\\), excludes category \\(j\\). example, \\(\\theta\\) sample mean, \\(\\hat{\\theta}_{-s_j} = \\bar{X}_{-s_j}\\).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"derivation-of-error-measures","dir":"Articles","previous_headings":"Group-level cross-validation for data cubes","what":"Derivation of error measures","title":"Group-Level Sensitivity Analysis","text":", can calculate different error measures help inform analyses decision-making steps. Error Measures: Error difference statistic estimated without category \\(j\\) (\\(\\hat{\\theta}_{-s_j}\\)) statistic calculated complete dataset (\\(\\hat{\\theta}\\)). \\[ \\text{Error}_{s_j} = \\hat{\\theta}_{-s_j} - \\hat{\\theta} \\] Relative Error absolute error, normalized true estimate \\(\\hat{\\theta}\\) small error term \\(\\epsilon = 10^{-8}\\) avoid division zero. \\[ \\text{Rel. Error}_{s_j} = \\frac{|\\hat{\\theta}_{-s_j} - \\hat{\\theta}|}{\\hat{\\theta} +\\epsilon} \\] Percent Error relative error expressed percentage. \\[ \\text{Perc. Error}_{s_j} = \\text{Rel. Error}_{s_j} \\times 100 \\% \\] Summary Measures: Mean Relative Error (MRE) average relative errors categories. \\[ \\text{MRE} = \\frac{1}{s} \\sum_{j=1}^s \\text{Rel. Error}_{s_j} \\] Mean Squared Error (MSE) average squared errors. \\[ \\text{MSE} = \\frac{1}{s} \\sum_{j=1}^s (\\text{Error}_{s_j})^2 \\] Root Mean Squared Error (RMSE) square root MSE. \\[ \\text{RMSE} = \\sqrt{\\text{MSE}} \\]","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"getting-started-with-dubicube","dir":"Articles","previous_headings":"","what":"Getting started with dubicube","title":"Group-Level Sensitivity Analysis","text":"dubicube cross-validation method can applied dataframe statistic calculated grouping variable present. tutorial, focus occurrence cubes. Therefore, use b3gbi package processing raw data moving cross-validation.","code":"# Load packages library(ggplot2)      # Data visualisation library(dplyr)        # Data wrangling  # Data loading and processing library(frictionless) # Load example datasets library(b3gbi)        # Process occurrence cubes library(dubicube)     # Analysis of data quality & reliability"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"loading-and-processing-the-data","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Loading and processing the data","title":"Group-Level Sensitivity Analysis","text":"load bird cube data b3data data package using frictionless (see also ). occurrence cube birds Belgium 2000 en 2024 using MGRS grid 10 km scale. process cube b3gbi put data format, ready analysis. select data 2011 - 2020.","code":"# Read data package b3data_package <- read_package(   \"https://zenodo.org/records/15211029/files/datapackage.json\" )  # Load bird cube data bird_cube_belgium <- read_resource(b3data_package, \"bird_cube_belgium_mgrs10\") head(bird_cube_belgium) #> # A tibble: 6 × 8 #>    year mgrscode specieskey species          family     n mincoordinateuncerta…¹ #>   <dbl> <chr>         <dbl> <chr>            <chr>  <dbl>                  <dbl> #> 1  2000 31UDS65     2473958 Perdix perdix    Phasi…     1                   3536 #> 2  2000 31UDS65     2474156 Coturnix coturn… Phasi…     1                   3536 #> 3  2000 31UDS65     2474377 Fulica atra      Ralli…     5                   1000 #> 4  2000 31UDS65     2475443 Merops apiaster  Merop…     6                   1000 #> 5  2000 31UDS65     2480242 Vanellus vanell… Chara…     1                   3536 #> 6  2000 31UDS65     2480637 Accipiter nisus  Accip…     1                   3536 #> # ℹ abbreviated name: ¹​mincoordinateuncertaintyinmeters #> # ℹ 1 more variable: familycount <dbl> processed_cube <- process_cube(   bird_cube_belgium,   first_year = 2011,   last_year = 2020,   cols_occurrences = \"n\" ) processed_cube #>  #> Processed data cube for calculating biodiversity indicators #>  #> Date Range: 2011 - 2020  #> Single-resolution cube with cell size 10km ^2  #> Number of cells: 379  #> Grid reference system: mgrs  #> Coordinate range: #>    xmin    xmax    ymin    ymax  #>  280000  710000 5480000 5700000  #>  #> Total number of observations: 13225290  #> Number of species represented: 646  #> Number of families represented: 92  #>  #> Kingdoms represented: Data not present  #>  #> First 10 rows of data (use n = to show more): #>  #> # A tibble: 280,184 × 13 #>     year cellCode taxonKey scientificName    family   obs minCoordinateUncerta…¹ #>    <dbl> <chr>       <dbl> <chr>             <chr>  <dbl>                  <dbl> #>  1  2011 31UDS65   2474051 Alectoris rufa    Phasi…     1                    100 #>  2  2011 31UDS65   2474377 Fulica atra       Ralli…     6                   1000 #>  3  2011 31UDS65   2474831 Rallus aquaticus  Ralli…     1                   1000 #>  4  2011 31UDS65   2478523 Picus viridis     Picid…     5                   3536 #>  5  2011 31UDS65   2480242 Vanellus vanellus Chara…     4                   1000 #>  6  2011 31UDS65   2480332 Pluvialis aprica… Chara…     1                   1000 #>  7  2011 31UDS65   2480482 Circus aeruginos… Accip…     2                   3536 #>  8  2011 31UDS65   2480487 Circus cyaneus    Accip…     9                   3536 #>  9  2011 31UDS65   2480537 Buteo buteo       Accip…     8                   3536 #> 10  2011 31UDS65   2480637 Accipiter nisus   Accip…     9                   3536 #> # ℹ 280,174 more rows #> # ℹ abbreviated name: ¹​minCoordinateUncertaintyInMeters #> # ℹ 6 more variables: familyCount <dbl>, xcoord <dbl>, ycoord <dbl>, #> #   utmzone <int>, hemisphere <chr>, resolution <chr>"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"analysis-of-the-data","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Analysis of the data","title":"Group-Level Sensitivity Analysis","text":"Suppose interested average number observations per grid cell per year. create function mean_obs() calculate . get following results: values tell us anything reliability . Therefore, perform leave-one-species-cross-validation (LOSO-CV) investigate impact individual species excluded calculation.","code":"# Function to calculate the mean of observations per grid cell per year mean_obs <- function(data) {   obs <- x <- NULL    data %>%     dplyr::mutate(x = mean(obs), .by = \"cellCode\") %>%     dplyr::summarise(diversity_val = mean(x), .by = \"year\") %>%     as.data.frame() } mean_obs(processed_cube$data) #>    year diversity_val #> 1  2011      48.83864 #> 2  2012      48.27942 #> 3  2013      48.13675 #> 4  2014      47.82226 #> 5  2015      47.39197 #> 6  2016      47.32866 #> 7  2017      46.31520 #> 8  2018      45.87515 #> 9  2019      45.93329 #> 10 2020      44.98668"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"leave-one-species-out-cross-validation","dir":"Articles","previous_headings":"Getting started with dubicube","what":"Leave-one-species-out cross-validation","title":"Group-Level Sensitivity Analysis","text":"use cross_validate_cube() function . relies following arguments: data_cube: input data, either processed data cube (b3gbi::process_cube()), just dataframe inside (.e. processed_cube$data). faster computation, passing just dataframe recommended. fun: user-defined function computes statistic(s) interest data_cube. function return dataframe includes column named diversity_val, containing statistic evaluate. grouping_var: column(s) used grouping output fun(). example, fun() returns one value per year, use grouping_var = \"year\". out_var: variable used leave one group time cross-validation. default \"taxonKey\" (typically representing species), results leave-one-species-cross-validation. progress: Logical flag show progress bar. Set TRUE enable progress reporting; default FALSE. RMSE average error measure obtain year. remains similar years.  Indeed, looking individual error values reveals similar patterns every year. species taxon key 2481174 error -2 year 2011. means without species average number occurrences per grid cell 2 units lower estimate based full dataset (case around 46.8 instead 48.8).  two species big effect calculation statistic. 2481174: Larus fuscus Linnaeus, 1758 2481139: Larus argentatus Pontoppidan, 1763 go back source data, see data cube includes records two large datasets strictly dedicated two species (2481174, 2481139). suggests two species overrepresented data cube, likely due targeted monitoring efforts. disproportionate influence inflates indicator values, potentially distorting true diversity signal. Group-level sensitivity analysis like helps uncover sampling biases, ensuring biodiversity indicators reflect ecological reality rather artifacts data.","code":"cv_results <- cross_validate_cube(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   out_var = \"taxonKey\" ) head(cv_results) #>   id_cv year taxonkey_out   rep_cv est_original       error     sq_error #> 1     1 2011      2474051 48.85551     48.83864  0.01686866 0.0002845518 #> 2     2 2011      2474377 48.49272     48.83864 -0.34592150 0.1196616847 #> 3     3 2011      2474831 48.91447     48.83864  0.07583503 0.0057509524 #> 4     4 2011      2478523 48.66861     48.83864 -0.17002678 0.0289091060 #> 5     5 2011      2480242 48.56884     48.83864 -0.26979464 0.0727891466 #> 6     6 2011      2480332 48.97265     48.83864  0.13401280 0.0179594315 #>    abs_error    rel_error perc_error         mre        mse      rmse #> 1 0.01686866 0.0003453959 0.03453959 0.001122516 0.03544285 0.1882627 #> 2 0.34592150 0.0070829473 0.70829473 0.001122516 0.03544285 0.1882627 #> 3 0.07583503 0.0015527672 0.15527672 0.001122516 0.03544285 0.1882627 #> 4 0.17002678 0.0034813989 0.34813989 0.001122516 0.03544285 0.1882627 #> 5 0.26979464 0.0055242048 0.55242048 0.001122516 0.03544285 0.1882627 #> 6 0.13401280 0.0027439914 0.27439914 0.001122516 0.03544285 0.1882627 # Visualise mean errors ggplot(cv_results, aes(x = as.factor(year))) +   # Reference line   geom_hline(yintercept = 0, colour = \"black\", linetype = \"dashed\") +   # Plot RMSE   geom_point(aes(y = rmse), colour = \"red\", size = 3) +   # Settings   labs(x = \"\", y = \"RMSE\") +   theme_minimal() # Get original estimates (based on full data cube) original_estimates <- mean_obs(processed_cube$data) original_estimates$label <- paste(   \"hat(theta) ==\", round(original_estimates$diversity_val, 3) )  # Visualise errors ggplot(cv_results, aes(x = as.factor(year))) +   # Reference line   geom_hline(yintercept = 0, colour = \"black\", linetype = \"dashed\") +   # Plot species keys   geom_text(aes(y = error, colour = as.factor(taxonkey_out),                 label = as.factor(taxonkey_out))) +   # Plot original estimates   geom_label(data = original_estimates, aes(label = label),              y = 0.9, parse = TRUE, size = 3) +   # Settings   scale_y_continuous(limits = c(NA, 1)) +   labs(x = \"\", y = \"LOSO-CV Error\") +   theme_minimal() +   theme(legend.position = \"\") index <- match(\"bird_cube_belgium_mgrs10\", resources(b3data_package)) b3data_package$resources[[index]]$sources #> [[1]] #> [[1]]$title #> [1] \"GBIF Occurrence Download\" #>  #> [[1]]$path #> [1] \"https://doi.org/10.15468/dl.y3wpwk\""},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"cross-validate-simple-dataframes","dir":"Articles","previous_headings":"Advanced usage of cross_validate_cube()","what":"Cross-validate simple dataframes","title":"Group-Level Sensitivity Analysis","text":"stated documentation, also possible cross-validate dataframe. case, set argument processed_cube = FALSE. implemented allow flexible use simple dataframes, still encouraging use b3gbi::process_cube() default functionality.","code":"cv_results_df <- cross_validate_cube(   data_cube = processed_cube$data,      # data.frame object   fun = mean_obs,   grouping_var = \"year\",   out_var = \"taxonKey\",   processed_cube = FALSE )"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/group-level-sensitivity.html","id":"optional-arguments","dir":"Articles","previous_headings":"Advanced usage of cross_validate_cube()","what":"Optional arguments","title":"Group-Level Sensitivity Analysis","text":"cross_validate_cube() function includes several optional parameters give control cross-validation performed. arguments required basic use, can helpful specific scenarios. Cross-validation Method: crossv_method Determines data partitioned cross-validation. \"loo\" (default): Leave-one-cross-validation. One category out_var (e.g. one species) excluded time. \"kfold\": K-fold cross-validation. categories out_var split k subsets, subset excluded others used analysis.Note: method experimental results interpreted caution. Number Folds: k Specifies number folds using \"kfold\" cross-validation. Default 5. used crossv_method = \"kfold\". Maximum Number Categories: max_out_cats Sets upper limit number unique categories out_var excluded one--one cross-validation. Default 1000. helps prevent long runtimes datasets many unique categories. can increase needed, expect slower computation.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-spatial-trends.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Visualising Spatial Trends","text":"tutorial provides good practices regarding visualisation interpretation trends indicators space. methods discussed broadly applicable, tutorial focus occurrence cubes biodiversity indicators derived.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-spatial-trends.html","id":"calculating-confidence-intervals-with-dubicube","dir":"Articles","previous_headings":"","what":"Calculating confidence intervals with dubicube","title":"Visualising Spatial Trends","text":"reuse example introduced bootstrap confidence interval calculation tutorial look occurrence cube birds Belgium 2000 en 2024 using MGRS grid 10 km scale. calculate confidence limits mean number observations per grid cell.","code":"# Load packages library(ggplot2)      # Data visualisation library(dplyr)        # Data wrangling library(sf)           # Work with spatial objects  # Data loading and processing library(frictionless) # Load example datasets library(b3gbi)        # Process occurrence cubes library(dubicube)     # Analysis of data quality & indicator uncertainty"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-spatial-trends.html","id":"loading-and-processing-the-data","dir":"Articles","previous_headings":"Calculating confidence intervals with dubicube","what":"Loading and processing the data","title":"Visualising Spatial Trends","text":"load bird cube data b3data data package using frictionless (see also ). process cube b3gbi. First, select 3000 random rows make dataset smaller. keep grid cells 10 entries. reduce computation time tutorial.","code":"# Read data package b3data_package <- read_package(   \"https://zenodo.org/records/15211029/files/datapackage.json\" )  # Load bird cube data bird_cube_belgium <- read_resource(b3data_package, \"bird_cube_belgium_mgrs10\") head(bird_cube_belgium) #> # A tibble: 6 × 8 #>    year mgrscode specieskey species          family     n mincoordinateuncerta…¹ #>   <dbl> <chr>         <dbl> <chr>            <chr>  <dbl>                  <dbl> #> 1  2000 31UDS65     2473958 Perdix perdix    Phasi…     1                   3536 #> 2  2000 31UDS65     2474156 Coturnix coturn… Phasi…     1                   3536 #> 3  2000 31UDS65     2474377 Fulica atra      Ralli…     5                   1000 #> 4  2000 31UDS65     2475443 Merops apiaster  Merop…     6                   1000 #> 5  2000 31UDS65     2480242 Vanellus vanell… Chara…     1                   3536 #> 6  2000 31UDS65     2480637 Accipiter nisus  Accip…     1                   3536 #> # ℹ abbreviated name: ¹​mincoordinateuncertaintyinmeters #> # ℹ 1 more variable: familycount <dbl> set.seed(123)  # Make dataset smaller rows <- sample(nrow(bird_cube_belgium), 3000) bird_cube_belgium <- bird_cube_belgium[rows, ] %>%   mutate(n_obs = n(), .by = \"mgrscode\") %>%   filter(n_obs > 10) %>%   select(-n_obs)  # Process cube processed_cube <- process_cube(   bird_cube_belgium,   cols_occurrences = \"n\" ) processed_cube #>  #> Processed data cube for calculating biodiversity indicators #>  #> Date Range: 2000 - 2024  #> Single-resolution cube with cell size 10km ^2  #> Number of cells: 134  #> Grid reference system: mgrs  #> Coordinate range: #>    xmin    xmax    ymin    ymax  #>  460000  690000 5610000 5700000  #>  #> Total number of observations: 91206  #> Number of species represented: 316  #> Number of families represented: 67  #>  #> Kingdoms represented: Data not present  #>  #> First 10 rows of data (use n = to show more): #>  #> # A tibble: 2,391 × 13 #>     year cellCode taxonKey scientificName    family   obs minCoordinateUncerta…¹ #>    <dbl> <chr>       <dbl> <chr>             <chr>  <dbl>                  <dbl> #>  1  2000 31UES44   2481714 Tringa totanus    Scolo…     1                   3536 #>  2  2000 31UFS05   2481740 Calidris temminc… Scolo…     3                   3536 #>  3  2000 31UES43   2492943 Sylvia communis   Sylvi…     8                   1414 #>  4  2000 31UES44   5739317 Phoenicurus phoe… Musci…    10                   1000 #>  5  2000 31UFS63   2481700 Scolopax rustico… Scolo…     1                   3536 #>  6  2000 31UFS74   5845582 Chloris chloris   Fring…     3                   3536 #>  7  2000 31UFS65   2492960 Sylvia curruca    Sylvi…     7                   3536 #>  8  2000 31UFS07   2493091 Phylloscopus col… Phyll…    19                   1414 #>  9  2000 31UDS86   2489214 Delichon urbicum  Hirun…     3                   3536 #> 10  2000 31UES85   2473958 Perdix perdix     Phasi…     9                   1414 #> # ℹ 2,381 more rows #> # ℹ abbreviated name: ¹​minCoordinateUncertaintyInMeters #> # ℹ 6 more variables: familyCount <dbl>, xcoord <dbl>, ycoord <dbl>, #> #   utmzone <int>, hemisphere <chr>, resolution <chr>"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-spatial-trends.html","id":"analysis-of-the-data","dir":"Articles","previous_headings":"Calculating confidence intervals with dubicube","what":"Analysis of the data","title":"Visualising Spatial Trends","text":"Let’s say interested mean number observations per grid cell. create function calculate . get following results: , values don’t reveal much uncertainty surrounds . better understand variability, use bootstrapping estimate distribution yearly means. , can calculate bootstrap confidence intervals.","code":"# Function to calculate statistic of interest # Mean observations per grid cell mean_obs_grid <- function(data) {   data %>%     dplyr::summarise(diversity_val = mean(obs), .by = \"cellCode\") %>%     as.data.frame() } head(   mean_obs_grid(processed_cube$data) ) #>   cellCode diversity_val #> 1  31UES44      50.10526 #> 2  31UFS05      26.04762 #> 3  31UES43      35.42105 #> 4  31UFS63      16.93750 #> 5  31UFS74      24.94444 #> 6  31UFS65      26.17647"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-spatial-trends.html","id":"bootstrapping","dir":"Articles","previous_headings":"Calculating confidence intervals with dubicube","what":"Bootstrapping","title":"Visualising Spatial Trends","text":"use bootstrap_cube() function perform bootstrapping (see also bootstrap tutorial). Since indicator calculated independently per group (grid cell) perform group-specific bootstrapping. detailed discussion use approach, see tutorial.","code":"bootstrap_results <- processed_cube$data %>%   split(processed_cube$data$cellCode) %>%   lapply(function(cube) {     bootstrap_results <- bootstrap_cube(       data_cube = cube,       fun = mean_obs_grid,       grouping_var = \"cellCode\",       samples = 1000,       seed = 123,       processed_cube = FALSE     )      return(list(bootstrap_results = bootstrap_results, data = cube))   }) head(bootstrap_results[[1]]$bootstrap_results) #>   sample cellCode est_original rep_boot est_boot   se_boot bias_boot #> 1      1  31UDS65     3.285714 4.357143 3.263714 0.9057524    -0.022 #> 2      2  31UDS65     3.285714 2.714286 3.263714 0.9057524    -0.022 #> 3      3  31UDS65     3.285714 3.571429 3.263714 0.9057524    -0.022 #> 4      4  31UDS65     3.285714 3.714286 3.263714 0.9057524    -0.022 #> 5      5  31UDS65     3.285714 3.214286 3.263714 0.9057524    -0.022 #> 6      6  31UDS65     3.285714 2.785714 3.263714 0.9057524    -0.022"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-spatial-trends.html","id":"interval-calculation","dir":"Articles","previous_headings":"Calculating confidence intervals with dubicube","what":"Interval calculation","title":"Visualising Spatial Trends","text":"Now can use calculate_bootstrap_ci() function calculate confidence limits (see also bootstrap confidence interval calculation tutorial). get warning message BCa calculation using relatively small dataset.","code":"ci_mean_obs_list <- bootstrap_results %>%   lapply(function(list) {     calculate_bootstrap_ci(       list$bootstrap_results,       grouping_var = \"cellCode\",       type = c(\"perc\", \"bca\", \"norm\", \"basic\"),       conf = 0.95,       data_cube = list$data, # Required for Bca       fun = mean_obs_grid    # Required for Bca     )   }) #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints.  # Make interval type factor ci_mean_obs <- bind_rows(ci_mean_obs_list) %>%   mutate(     int_type = factor(       int_type, levels = c(\"perc\", \"bca\", \"norm\", \"basic\")     )   ) head(ci_mean_obs) #>   cellCode est_original  est_boot   se_boot bias_boot int_type conf       ll #> 1  31UDS65     3.285714  3.263714 0.9057524   -0.0220     perc 0.95 1.785714 #> 2  31UDS65     3.285714  3.263714 0.9057524   -0.0220      bca 0.95 2.000000 #> 3  31UDS65     3.285714  3.263714 0.9057524   -0.0220     norm 0.95 1.532472 #> 4  31UDS65     3.285714  3.263714 0.9057524   -0.0220    basic 0.95 1.357143 #> 5  31UDS66    10.333333 10.357533 6.1848863    0.0242     perc 0.95 2.400000 #> 6  31UDS66    10.333333 10.357533 6.1848863    0.0242      bca 0.95 3.066667 #>          ul #> 1  5.214286 #> 2  6.434174 #> 3  5.082956 #> 4  4.785714 #> 5 24.000000 #> 6 36.095861"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-spatial-trends.html","id":"visualising-uncertainty-in-spatial-trends","dir":"Articles","previous_headings":"","what":"Visualising uncertainty in spatial trends","title":"Visualising Spatial Trends","text":"can visualise estimate confidence levels separate figures.    want visualise estimates uncertainty single figure, need good uncertainty measure. One straightforward option width confidence interval (CI): \\[ \\text{CI width} = \\text{upper limit} - \\text{lower limit} \\] directly reflects uncertainty — wider intervals indicate greater uncertainty. allow comparisons across spatial units different magnitudes, may prefer relative measure uncertainty relative CI half-width, calculated : \\[ \\frac{\\text{CI width}}{2 \\times \\text{estimate}} \\] expresses margin error proportion estimate, easier interpret. example, value 0.1 implies ±10% uncertainty around point estimate (assuming symmetric intervals). Alternatively, can use bootstrap standard error measure uncertainty. Similar CI width, can expressed absolute relative terms (e.g., standard error divided estimate) depending whether want visualise raw normalized uncertainty. visualising estimate uncertainty single map, can use circles within grid cells vary transparency (best w.r.t. user performance ~ accuracy, speed), blurriness (best w.r.t. user intuitiveness) (Kinkeldey et al., 2014; MacEachren et al., 2005, 2012).","code":"# Read MGRS grid from repository mgrs10_belgium <- st_read(   \"https://zenodo.org/records/15211029/files/mgrs10_refgrid_belgium.gpkg\",   quiet = TRUE )  # Get BCa intervals bca_mean_obs <- ci_mean_obs %>%   filter(int_type == \"bca\") %>%   # Add MGRS grid   left_join(mgrs10_belgium, by = join_by(cellCode == mgrscode)) %>%   st_sf(sf_column_name = \"geom\", crs = st_crs(mgrs10_belgium)) # Visualise estimates bca_mean_obs %>%   # Visualise result   ggplot() +   geom_sf(data = mgrs10_belgium) +   geom_sf(aes(fill = est_original)) +   # Settings   scale_fill_viridis_c(option = \"D\") +   labs(title = \"Estimate\", fill = \"Legend\") +   theme_minimal() # Visualise lower CI's bca_mean_obs %>%   # Visualise result   ggplot() +   geom_sf(data = mgrs10_belgium) +   geom_sf(aes(fill = ll)) +   # Settings   scale_fill_viridis_c(option = \"D\") +   labs(title = \"Lower confidence limit\", fill = \"Legend\") +   theme_minimal() # Visualise upper CI's bca_mean_obs %>%   # Visualise result   ggplot() +   geom_sf(data = mgrs10_belgium) +   geom_sf(aes(fill = ul)) +   # Settings   scale_fill_viridis_c(option = \"D\") +   labs(title = \"Upper confidence limit\", fill = \"Legend\") +   theme_minimal()"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-spatial-trends.html","id":"transparency","dir":"Articles","previous_headings":"Visualising uncertainty in spatial trends","what":"Transparency","title":"Visualising Spatial Trends","text":"Let’s visualise relative half-width use larger transparency larger uncertainty. Transparency can scaled using scale_alpha() function ggplot2.  make visualisation even clear, can also vary size based uncertainty measure. Size can scaled using scale_size() function ggplot2.","code":"# Calculate center points st_centroid(bca_mean_obs) %>%   mutate(x = st_coordinates(geom)[, 1],          y = st_coordinates(geom)[, 2],          # Calculate uncertainty measure          uncertainty = (ul - ll) /  (2 * est_original)) %>%   # Visualise   ggplot() +   geom_sf(data = mgrs10_belgium) +   geom_point(     aes(x = x, y = y, colour = est_original, alpha = uncertainty),     size = 5   ) +   # Settings   scale_colour_viridis_c(option = \"D\") +   scale_alpha(range = c(1, 0.3)) +    # Scale accordingly   labs(colour = \"Estimate\", alpha = \"Uncertainty\",        x = \"\", y = \"\") +   theme_minimal() # Calculate center points st_centroid(bca_mean_obs) %>%   mutate(x = st_coordinates(geom)[, 1],          y = st_coordinates(geom)[, 2],          # Calculate uncertainty measure          uncertainty = (ul - ll) /  (2 * est_original)) %>%   # Visualise   ggplot() +   geom_sf(data = mgrs10_belgium) +   geom_point(     aes(x = x, y = y, colour = est_original, alpha = uncertainty,         size = uncertainty)   ) +   # Settings   scale_colour_viridis_c(option = \"D\") +   scale_alpha(range = c(1, 0.3)) +    # Scale accordingly   scale_size(range = c(5, 2)) +       # Scale accordingly   labs(colour = \"Estimate\", alpha = \"Uncertainty\", size = \"Uncertainty\",        x = \"\", y = \"\") +   theme_minimal()"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-spatial-trends.html","id":"blurriness","dir":"Articles","previous_headings":"Visualising uncertainty in spatial trends","what":"Blurriness","title":"Visualising Spatial Trends","text":"Unlike transparency point size, blurriness natively supported ggplot2. Therefore, present custom figure using hard-coded example illustrates difference blurriness transparency visual indicators spatial uncertainty.  figure created using R packages ggplot2, dplyr, sf, ggblur. ggblur package provides useful starting point implementing blur effects ggplot2 plots, fully meet requirements. ggblur, blurriness simulated plotting original point together series increasingly larger transparent copies behind . creates visual “halo” effect mimics blur. However, ggblur increases size transparency blurred copies simultaneously, whereas require flexibility: maximum size blur able remain constant even decrease, perceived blur increases. achieve controlled flexible behaviour, need develop new, dedicated R package allows finer control relationship size blur.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-spatial-trends.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Visualising Spatial Trends","text":"Kinkeldey, C., MacEachren, . M., & Schiewe, J. (2014). Assess Visual Communication Uncertainty? Systematic Review Geospatial Uncertainty Visualisation User Studies. Cartographic Journal, 51(4), 372–386. https://doi.org/10.1179/1743277414Y.0000000099 MacEachren, . M., Robinson, ., Hopper, S., Gardner, S., Murray, R., Gahegan, M., & Hetzler, E. (2005). Visualizing Geospatial Information Uncertainty: Know Need Know. Cartography Geographic Information Science, 32(3), 139–160. https://doi.org/10.1559/1523040054738936 MacEachren, . M., Roth, R. E., O’Brien, J., Li, B., Swingley, D., & Gahegan, M. (2012). Visual Semiotics & Uncertainty Visualization: Empirical Study. IEEE Transactions Visualization Computer Graphics, 18(12), 2496–2505. https://doi.org/10.1109/TVCG.2012.279","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Visualising Temporal Trends","text":"tutorial provides good practices regarding visualisation interpretation trends indicators time. methods discussed broadly applicable, tutorial focus occurrence cubes biodiversity indicators derived. visualisation interpretation, strongly rely functionality concepts effectclass package.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"calculating-confidence-intervals-with-dubicube","dir":"Articles","previous_headings":"","what":"Calculating confidence intervals with dubicube","title":"Visualising Temporal Trends","text":"reuse example introduced bootstrap confidence interval calculation tutorial calculate confidence limits mean number observations per grid cell per year birds Belgium 2011 en 2020 using MGRS grid 10 km scale.","code":"# Load packages library(ggplot2)      # Data visualisation library(dplyr)        # Data wrangling library(tidyr)        # Data wrangling  # Data loading and processing library(frictionless) # Load example datasets library(b3gbi)        # Process occurrence cubes library(dubicube)     # Analysis of data quality & indicator uncertainty"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"loading-and-processing-the-data","dir":"Articles","previous_headings":"Calculating confidence intervals with dubicube","what":"Loading and processing the data","title":"Visualising Temporal Trends","text":"load bird cube data b3data data package using frictionless (see also ). process cube b3gbi. First, select 2000 random rows make dataset smaller. reduce computation time tutorial. select data 2011 - 2020.","code":"# Read data package b3data_package <- read_package(   \"https://zenodo.org/records/15211029/files/datapackage.json\" )  # Load bird cube data bird_cube_belgium <- read_resource(b3data_package, \"bird_cube_belgium_mgrs10\") head(bird_cube_belgium) #> # A tibble: 6 × 8 #>    year mgrscode specieskey species          family     n mincoordinateuncerta…¹ #>   <dbl> <chr>         <dbl> <chr>            <chr>  <dbl>                  <dbl> #> 1  2000 31UDS65     2473958 Perdix perdix    Phasi…     1                   3536 #> 2  2000 31UDS65     2474156 Coturnix coturn… Phasi…     1                   3536 #> 3  2000 31UDS65     2474377 Fulica atra      Ralli…     5                   1000 #> 4  2000 31UDS65     2475443 Merops apiaster  Merop…     6                   1000 #> 5  2000 31UDS65     2480242 Vanellus vanell… Chara…     1                   3536 #> 6  2000 31UDS65     2480637 Accipiter nisus  Accip…     1                   3536 #> # ℹ abbreviated name: ¹​mincoordinateuncertaintyinmeters #> # ℹ 1 more variable: familycount <dbl> set.seed(123)  # Make dataset smaller rows <- sample(nrow(bird_cube_belgium), 2000) bird_cube_belgium <- bird_cube_belgium[rows, ]  # Process cube processed_cube <- process_cube(   bird_cube_belgium,   first_year = 2011,   last_year = 2020,   cols_occurrences = \"n\" ) processed_cube #>  #> Processed data cube for calculating biodiversity indicators #>  #> Date Range: 2011 - 2020  #> Single-resolution cube with cell size 10km ^2  #> Number of cells: 242  #> Grid reference system: mgrs  #> Coordinate range: #>    xmin    xmax    ymin    ymax  #>  280000  710000 5490000 5700000  #>  #> Total number of observations: 45143  #> Number of species represented: 253  #> Number of families represented: 57  #>  #> Kingdoms represented: Data not present  #>  #> First 10 rows of data (use n = to show more): #>  #> # A tibble: 957 × 13 #>     year cellCode taxonKey scientificName    family   obs minCoordinateUncerta…¹ #>    <dbl> <chr>       <dbl> <chr>             <chr>  <dbl>                  <dbl> #>  1  2011 31UFS56   5231918 Cuculus canorus   Cucul…    11                   3536 #>  2  2011 31UES28   5739317 Phoenicurus phoe… Musci…     6                   3536 #>  3  2011 31UFS64   6065824 Chroicocephalus … Larid…   143                   1000 #>  4  2011 31UFS96   2492576 Muscicapa striata Musci…     3                   3536 #>  5  2011 31UES04   5231198 Passer montanus   Passe…     1                   3536 #>  6  2011 31UES85   5229493 Garrulus glandar… Corvi…    23                    707 #>  7  2011 31UES88  10124612 Anser anser x Br… Anati…     1                    100 #>  8  2011 31UES22   2481172 Larus marinus     Larid…     8                   1000 #>  9  2011 31UFS43   2481139 Larus argentatus  Larid…    10                   3536 #> 10  2011 31UFT00   9274012 Spatula querqued… Anati…     8                   3536 #> # ℹ 947 more rows #> # ℹ abbreviated name: ¹​minCoordinateUncertaintyInMeters #> # ℹ 6 more variables: familyCount <dbl>, xcoord <dbl>, ycoord <dbl>, #> #   utmzone <int>, hemisphere <chr>, resolution <chr>"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"analysis-of-the-data","dir":"Articles","previous_headings":"Calculating confidence intervals with dubicube","what":"Analysis of the data","title":"Visualising Temporal Trends","text":"Let’s say interested mean number observations per grid cell per year. create function calculate . get following results: , values don’t reveal much uncertainty surrounds . better understand variability, use bootstrapping estimate distribution yearly means. , can calculate bootstrap confidence intervals.","code":"# Function to calculate statistic of interest # Mean observations per grid cell per year mean_obs <- function(data) {   data %>%     dplyr::mutate(x = mean(obs), .by = \"cellCode\") %>%     dplyr::summarise(diversity_val = mean(x), .by = \"year\") %>%     as.data.frame() } mean_obs(processed_cube$data) #>    year diversity_val #> 1  2011      34.17777 #> 2  2012      35.27201 #> 3  2013      33.25581 #> 4  2014      55.44160 #> 5  2015      49.24754 #> 6  2016      48.34063 #> 7  2017      70.42202 #> 8  2018      48.83850 #> 9  2019      47.46795 #> 10 2020      43.00411"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"bootstrapping","dir":"Articles","previous_headings":"Calculating confidence intervals with dubicube","what":"Bootstrapping","title":"Visualising Temporal Trends","text":"use bootstrap_cube() function perform bootstrapping (see also bootstrap tutorial).","code":"bootstrap_results <- bootstrap_cube(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   samples = 1000,   seed = 123 ) head(bootstrap_results) #>   sample year est_original rep_boot est_boot se_boot  bias_boot #> 1      1 2011     34.17777 24.23133 33.80046 4.24489 -0.3773142 #> 2      2 2011     34.17777 24.28965 33.80046 4.24489 -0.3773142 #> 3      3 2011     34.17777 31.81445 33.80046 4.24489 -0.3773142 #> 4      4 2011     34.17777 33.42530 33.80046 4.24489 -0.3773142 #> 5      5 2011     34.17777 35.03502 33.80046 4.24489 -0.3773142 #> 6      6 2011     34.17777 33.72037 33.80046 4.24489 -0.3773142"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"interval-calculation","dir":"Articles","previous_headings":"Calculating confidence intervals with dubicube","what":"Interval calculation","title":"Visualising Temporal Trends","text":"Now can use calculate_bootstrap_ci() function calculate confidence limits (see also bootstrap confidence interval calculation tutorial). get warning message BCa calculation using relatively small dataset.","code":"ci_mean_obs <- calculate_bootstrap_ci(   bootstrap_samples_df = bootstrap_results,   grouping_var = \"year\",   type = c(\"perc\", \"bca\", \"norm\", \"basic\"),   conf = 0.95,   data_cube = processed_cube,   # Required for BCa   fun = mean_obs                # Required for BCa ) #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints.  # Make interval type factor ci_mean_obs <- ci_mean_obs %>%   mutate(     int_type = factor(       int_type, levels = c(\"perc\", \"bca\", \"norm\", \"basic\")     )   ) head(ci_mean_obs) #>   year est_original est_boot   se_boot  bias_boot int_type conf       ll #> 1 2011     34.17777 33.80046  4.244890 -0.3773142     perc 0.95 26.57582 #> 2 2012     35.27201 34.45877  3.776430 -0.8132403     perc 0.95 27.23367 #> 3 2013     33.25581 33.72937  5.351881  0.4735622     perc 0.95 24.91414 #> 4 2014     55.44160 53.13004 10.597359 -2.3115608     perc 0.95 36.04170 #> 5 2015     49.24754 48.60624  9.888400 -0.6412965     perc 0.95 34.60635 #> 6 2016     48.34063 47.10668 11.388627 -1.2339492     perc 0.95 30.82583 #>         ul #> 1 42.77975 #> 2 41.66334 #> 3 46.62692 #> 4 77.37950 #> 5 73.13698 #> 6 73.16598"},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"error-bars","dir":"Articles","previous_headings":"Visualising uncertainty in temporal trends","what":"Error bars","title":"Visualising Temporal Trends","text":"use error bars easy straightforward way visualise uncertainty around indicator estimates.  However, question remains interval types calculated /reported. good idea compare different interval types next together bootstrap distribution bootstrap bias (difference estimate bootstrap estimate).  informs us shape bootstrap distribution amount bootstrap bias. combination bootstrap interval theory, can decided interval type(s) reported. general, wide range biodiversity indicator types, recommend use percentile BCa intervals, strong assumptions regarding bootstrap distribution. BCa interval recommended accounts bias skewness. However, due jackknife estimation acceleration parameter, calculation time significantly longer. use normal basic confidence intervals recommended, used combination truncations transformations. assumption normality can checked making Q-Q plot bootstrap replications (see ) (Davison & Hinkley, 1997). overview recommendations provided table . exhaustive review topic, based existing literature preliminary results, recommendations provide useful starting point selecting appropriate interval types. Simplicity Understanding bootstrap CI theory Assumes bootstrap distribution normal Often, transformation needed accurate results Erratic coverage error practice Simplicity Understanding bootstrap CI theory Assumes symmetric bootstrap distribution Typically substantial coverage error Simplicity assumptions bootstrap distribution Implicitly uses existence good transformation take bias account Substantial coverage error distribution nearly symmetric assumptions bootstrap distribution Implicitly uses existence good transformation Adjusts bias Adjusts skewness Smaller coverage error methods Involved calculation acceleration parameter Unstable coverage sample size small small","code":"ci_mean_obs %>%   ggplot(aes(x = year, y = est_original)) +   # Intervals   geom_errorbar(aes(ymin = ll, ymax = ul),                 position = position_dodge(0.8), linewidth = 0.8) +   # Estimates   geom_point(colour = \"firebrick\", size = 2) +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results$year))) +   theme_minimal() +   facet_wrap(~int_type) # Get bias vales bias_mean_obs <- bootstrap_results %>%   distinct(year, estimate = est_original, `bootstrap estimate` = est_boot)  # Get estimate values estimate_mean_obs <- bias_mean_obs %>%   pivot_longer(cols = c(\"estimate\", \"bootstrap estimate\"),                names_to = \"Legend\", values_to = \"value\") %>%   mutate(Legend = factor(Legend, levels = c(\"estimate\", \"bootstrap estimate\"),                          ordered = TRUE)) # Visualise bootstrap_results %>%   ggplot(aes(x = year)) +   # Distribution   geom_violin(aes(y = rep_boot, group = year),               fill = alpha(\"cornflowerblue\", 0.2)) +   # Estimates and bias   geom_point(data = estimate_mean_obs, aes(y = value, shape = Legend),              colour = \"firebrick\", size = 2, alpha = 0.5) +   # Intervals   geom_errorbar(data = ci_mean_obs,                 aes(ymin = ll, ymax = ul, colour = int_type),                 position = position_dodge(0.8), linewidth = 0.6) +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\",        x = \"\", shape = \"Legend:\", colour = \"Interval type:\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results$year))) +   theme_minimal() +   theme(legend.position = \"bottom\",         legend.title = element_text(face = \"bold\")) #> Warning: Using shapes for an ordinal variable is not advised"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"smooth-trends","dir":"Articles","previous_headings":"Visualising uncertainty in temporal trends","what":"Smooth trends","title":"Visualising Temporal Trends","text":"create continuous representation change time, can apply LOESS (Locally Estimated Scatterplot Smoothing) estimates confidence limits. smoothing technique fits local regressions across subsets data, producing flexible trend line helps visualize broader patterns retaining important details. noted may complicate interpretation results. Occurrence cubes definition categorical time (case year), smoothing things time data aggregated might lead different results compared similar analysis original data (cube made aggregation occurred). Therefore, recommend visualisation long time series make general interpretations trend time. example, actually small time frame show sake tutorial. Based discussion , select BCa interval see asymmetry bias bootstrap distribution later years. Note probably due selection 2000 random rows original dataset reduce computation time tutorial. Otherwise expect symmetrical distribution indicator.","code":"ci_mean_obs %>%   filter(int_type == \"bca\") %>%   ggplot(aes(x = year, y = est_original)) +   geom_smooth(     colour = alpha(\"blue\", 0.5),     linetype = \"solid\",     method = \"loess\",     formula = \"y ~ x\",     se = FALSE   ) +   geom_smooth(     aes(y = ul),     colour = alpha(\"lightsteelblue1\", 1),     linetype = \"dashed\",     method = \"loess\",     formula = \"y ~ x\",     se = FALSE   ) +   geom_smooth(     aes(y = ll),     colour = alpha(\"lightsteelblue1\", 1),     linetype = \"dashed\",     method = \"loess\",     formula = \"y ~ x\",     se = FALSE   ) +   geom_ribbon(     aes(ymin = predict(loess(ll ~ year)), ymax = predict(loess(ul ~ year))),     alpha = 0.2, fill = \"lightsteelblue1\"   ) +   # Intervals   geom_errorbar(aes(ymin = ll, ymax = ul),                 position = position_dodge(0.8), linewidth = 0.8) +   # Estimates   geom_point(colour = \"firebrick\", size = 3) +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results$year))) +   theme_minimal()"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"fan-plots","dir":"Articles","previous_headings":"Visualising uncertainty in temporal trends","what":"Fan plots","title":"Visualising Temporal Trends","text":"effectclass package provides stat_fan() function create fan plot intervals (see also effectclass tutorial). intervals based normal distribution transformation via link argument. Therefore, check assumption normality making Q-Q plot bootstrap replications.  expected, looks like bootstrap distributions normally distributed years. log-transform bootstrap replications, Q-Q plots look better.  therefore calculate log-transformed normal intervals compare BCa normal intervals. log-normal intervals look indeed correct normal intervals, although still differ BCa intervals.  Finally, can create fan plot assuming log-normal distribution. Note stat_fan(), link_sd standard error link scale, y natural scale. recalculate standard error log-transformed bootstrap replicates. also need account bias. noted previous section, year discrete variable data cube. therefore use geom = \"rect\" option.  can make use concept make fan plots interval types . calculate BCA log-normal intervals five coverages using loop. can visualise categorical way geom_rect() ggplot2.  can visualise continuous way geom_ribbon() ggplot2. Linear (like default stat_fan()):  Smooth (like previous paragraph):","code":"library(effectclass) ggplot(bootstrap_results, aes(sample = rep_boot)) +   # Q-Q plot   stat_qq() +   stat_qq_line(col = \"red\") +   # Settings   labs(x = \"Theoretical Quantiles (Standard Normal)\",        y = \"Sample Quantiles (Bootstrap Replicates)\") +   facet_wrap(~ year, ncol = 2, scales = \"free\") +   theme_minimal() ggplot(bootstrap_results, aes(sample = log(rep_boot))) +   # Q-Q plot   stat_qq() +   stat_qq_line(col = \"red\") +   # Settings   labs(x = \"Theoretical Quantiles (Standard Normal)\",        y = \"Sample Quantiles (Bootstrap Replicates)\") +   facet_wrap(~ year, ncol = 2, scales = \"free\") +   theme_minimal() ci_mean_obs_lognorm <- calculate_bootstrap_ci(   bootstrap_samples_df = bootstrap_results,   grouping_var = \"year\",   type = c(\"norm\"),   conf = 0.95,   h = log,   hinv = exp ) # Combine interval data ci_mean_obs_new <- ci_mean_obs %>%   filter(int_type %in% c(\"bca\", \"norm\")) %>%   bind_rows(ci_mean_obs_lognorm %>% mutate(int_type = \"log_norm\"))  # Visualise bootstrap_results %>%   ggplot(aes(x = year)) +   # Distribution   geom_violin(aes(y = rep_boot, group = year),               fill = alpha(\"cornflowerblue\", 0.2)) +   # Estimates and bias   geom_point(data = estimate_mean_obs, aes(y = value, shape = Legend),              colour = \"firebrick\", size = 2, alpha = 0.5) +   # Intervals   geom_errorbar(data = ci_mean_obs_new,                 aes(ymin = ll, ymax = ul, colour = int_type),                 position = position_dodge(0.8), linewidth = 0.6) +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\",        x = \"\", shape = \"Legend:\", colour = \"Interval type:\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results$year))) +   theme_minimal() +   theme(legend.position = \"bottom\",         legend.title = element_text(face = \"bold\")) bootstrap_results %>%   # Calculate standard error on link scale   mutate(rep_boot_log = log(rep_boot),          est_boot_log = mean(rep_boot_log), # Needed for bias calculation          se_boot_log = sd(rep_boot_log),          .by = \"year\") %>%   # Get unique estimates per year   distinct(year, est_original, est_boot_log, se_boot_log) %>%   # Calculate bias factor   mutate(bias_log = exp(est_boot_log - log(est_original))) %>%   # Visualise   ggplot(aes(x = year)) +   effectclass::stat_fan(     aes(y = est_original / bias_log, link_sd = se_boot_log),     link = \"log\", fill = \"cornflowerblue\", max_prob = 0.95,     geom = \"rect\"   ) +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\", x = \"\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results$year))) +   theme_minimal() # Set up coverage levels max_prob <- 0.95 step <- 0.2 coverages <- seq(max_prob, 1e-3, by = -step)  # Loop over coverages out_ci_list <- vector(mode = \"list\", length = length(coverages))  for (i in seq_along(coverages)) {   cov <- coverages[i]    # Calculate confidence limits for confidence level   ci_cov <- calculate_bootstrap_ci(     bootstrap_samples_df = bootstrap_results,     grouping_var = \"year\",     type = c(\"bca\", \"norm\"),     conf = cov,     h = log,     hinv = exp,     data_cube = processed_cube,   # Required for BCa     fun = mean_obs                # Required for BCa   )    out_ci_list[[i]] <- ci_cov } out_ci <- bind_rows(out_ci_list) %>%   mutate(int_type = ifelse(int_type == \"norm\", \"log_norm\", int_type)) # For visualisation of envelopes alpha_levels <- sapply(coverages, function(i) 1 - i / (i + step)) names(alpha_levels) <- as.character(coverages)  # Convert conf to character so it can match alpha_levels out_ci <- out_ci %>%   mutate(     conf_chr = as.character(conf),     year_num = as.numeric(as.factor(year)) # Numeric year for xmin/xmax   )  # Visualise bar_width <- 0.9  ggplot(out_ci, aes()) +   geom_rect(     aes(       xmin = year_num - bar_width / 2,       xmax = year_num + bar_width / 2,       ymin = ll,       ymax = ul,       fill = conf_chr,       alpha = conf_chr     )   ) +   # Colour ribbons   scale_alpha_manual(values = alpha_levels, name = \"conf\") +   scale_fill_manual(values = rep(\"cornflowerblue\", length(alpha_levels)),                     name = \"conf\") +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\", x = \"\") +   scale_x_continuous(     breaks = unique(out_ci$year_num),     labels = unique(out_ci$year)   ) +   facet_wrap(~int_type, ncol = 1) +   theme_minimal() ggplot(out_ci, aes(x = year)) +   geom_ribbon(aes(ymin = ll, ymax = ul, fill = conf_chr, alpha = conf_chr)) +   # Colour ribbons   scale_alpha_manual(values = alpha_levels, name = \"conf\") +   scale_fill_manual(values = rep(\"cornflowerblue\", length(alpha_levels)),                     name = \"conf\") +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\", x = \"\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results$year))) +   facet_wrap(~int_type, ncol = 1) +   theme_minimal() # Predict smooth ymin/ymax out_ci %>%   group_by(conf_chr, int_type) %>%   mutate(ymin = predict(loess(ll ~ year)),          ymax = predict(loess(ul ~ year))) %>%   ungroup() %>%   # Visualise   ggplot(aes(x = year)) +   geom_ribbon(     aes(ymin = ymin, ymax = ymax, fill = conf_chr, alpha = conf_chr)   ) +   # Colour ribbons   scale_alpha_manual(values = alpha_levels, name = \"conf\") +   scale_fill_manual(values = rep(\"cornflowerblue\", length(alpha_levels)),                     name = \"conf\") +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\", x = \"\") +   scale_x_continuous(breaks = sort(unique(bootstrap_results$year))) +   facet_wrap(~int_type, ncol = 1) +   theme_minimal()"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"visualising-temporal-effects","dir":"Articles","previous_headings":"","what":"Visualising temporal effects","title":"Visualising Temporal Trends","text":"tutorial, demonstrated classify effect sizes based confidence intervals. give visualisation options based effectclass package (see also effectclass tutorial). Let’s calculate BCa interval mean number observations per grid cell compared 2011 perform effect classification threshold 5. effectclass package provides stat_effect() function visualises effects colours symbols.  function, can also add symbols stat_fan(). Note choice reference year well considered. Keep mind comparisons made, motivation behind reference period. high low value reference period relative periods, e.g. exceptional bad good year, can affect magnitude direction calculated differences. Whether avoided , depends motivation behind choice research question. reference period can determined legislation, start monitoring campaign. specific research question can determine periods need compared. Furthermore, variability estimate reference period affects width confidence intervals differences. variable reference period propagate greater uncertainty. case GBIF data, data available recent years earlier years. case, make sense select last period reference period. way, also avoids arbitrariness choice reference period. compare previous situations current situation (last year), repeat comparison annually, example. Finally, comparing multiple indicators, recommend using consistent reference period maintain comparability","code":"# Bootstrapping bootstrap_results_ref <- bootstrap_cube(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   samples = 1000,   ref_group = 2011,   seed = 123 )  # Calculate confidence intervals ci_mean_obs_ref <- calculate_bootstrap_ci(   bootstrap_samples_df = bootstrap_results_ref,   grouping_var = \"year\",   type = \"bca\",   data_cube = processed_cube,   # Required for BCa   fun = mean_obs,               # Required for BCa   ref_group = 2011              # Required for BCa ) #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints. #> Warning in norm_inter(h(t), adj_alpha): Extreme order statistics used as #> endpoints.  # Perform effect classification result <- add_effect_classification(   df = ci_mean_obs_ref,   cl_columns = c(\"ll\", \"ul\"),   threshold = 5,   reference = 0 )  # View the result result #>   year est_original    est_boot   se_boot  bias_boot int_type conf          ll #> 1 2012    1.0942452  0.65831911  5.475053 -0.4359261      bca 0.95 -10.7621638 #> 2 2013   -0.9219589 -0.07108256  6.690590  0.8508764      bca 0.95 -12.8513851 #> 3 2014   21.2638321 19.32958556 10.982093 -1.9342466      bca 0.95   5.8187198 #> 4 2015   15.0697689 14.80578656 10.723767 -0.2639823      bca 0.95   2.5039329 #> 5 2016   14.1628569 13.30622198 12.131657 -0.8566350      bca 0.95  -1.7552656 #> 6 2017   36.2442462 43.13141123 23.943155  6.8871650      bca 0.95   7.1906529 #> 7 2018   14.6607335 14.98367972 10.715491  0.3229462      bca 0.95   0.3961534 #> 8 2019   13.2901788  8.46222485 13.048145 -4.8279539      bca 0.95  -0.5622564 #> 9 2020    8.8263369  5.37019562 13.084703 -3.4561413      bca 0.95  -5.3273172 #>         ul effect_code effect_code_coarse             effect effect_coarse #> 1 11.04344           ?                  ?            unknown       unknown #> 2 13.06589           ?                  ?            unknown       unknown #> 3 57.17064          ++                  +    strong increase      increase #> 4 58.08685           +                  +           increase      increase #> 5 52.49345          ?+                  ? potential increase       unknown #> 6 96.55713          ++                  +    strong increase      increase #> 7 48.95507           +                  +           increase      increase #> 8 69.67581          ?+                  ? potential increase       unknown #> 9 65.47783           ?                  ?            unknown       unknown ggplot(data = result, aes(x = year, y = est_original, ymin = ll, ymax = ul)) +   effectclass::stat_effect(reference = 0, threshold = 5) +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell Compared to 2011\",        x = \"\") +   scale_x_continuous(breaks = sort(unique(result$year))) +   theme_minimal()"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/visualising-temporal-trends.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Visualising Temporal Trends","text":"Carpenter, J., & Bithell, J. (2000). Bootstrap confidence intervals: , , ? practical guide medical statisticians. Statistics Medicine, 19(9), 1141–1164. https://doi.org/10.1002/(SICI)1097-0258(20000515)19:9<1141::AID-SIM479>3.0.CO;2-F Davison, . C., & Hinkley, D. V. (1997). Bootstrap Methods Application (1st ed.). Cambridge University Press. https://doi.org/10.1017/CBO9780511802843 Dixon, P. M. (2001). Bootstrap Jackknife: Describing Precision Ecological Indices. S. M. Scheiner & J. Gurevitch (Eds.), Design Analysis Ecological Experiments (Second Edition, pp. 267–288). Oxford University PressNew York, NY. https://doi.org/10.1093/oso/9780195131871.003.0014 Efron, B., & Tibshirani, R. J. (1994). Introduction Bootstrap (1st ed.). Chapman Hall/CRC. https://doi.org/10.1201/9780429246593 Hesterberg, T. C. (2015). Teachers Know Bootstrap: Resampling Undergraduate Statistics Curriculum. American Statistician, 69(4), 371–386. https://doi.org/10.1080/00031305.2015.1089789","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/whole-cube-versus-group-specific-bootstrap.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Whole-cube Bootstrap versus Group-specific Bootstrap","text":"calculating biodiversity indicators cube, often want confidence intervals (CIs) using bootstrapping. dubicube, bootstrapping can done two ways: Whole-cube bootstrapping: resampling rows cube, regardless grouping. Group-specific bootstrapping: resampling rows within group interest (e.g., species, year, habitat). choice two methods directly affects confidence intervals interpreted: indicators combine information across groups (e.g., community richness, turnover, multi-species metrics). require whole-cube bootstrapping preserve correlations. indicators calculated independently per group (e.g., species-specific year-specific metrics). , group-specific bootstrapping usually appropriate. tutorial, explain differences, discuss strengths limitations method, provide worked example group-specific bootstrapping correct choice.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/whole-cube-versus-group-specific-bootstrap.html","id":"whole-cube-bootstrap","dir":"Articles","previous_headings":"Introduction","what":"Whole-cube bootstrap","title":"Whole-cube Bootstrap versus Group-specific Bootstrap","text":"Definition: Resample rows cube, regardless species, year, grouping. Advantages: Preserves correlations groups (e.g., species co-occurrence, temporal dependencies). Appropriate indicators depend multiple groups together (community-level metrics, multi-species diversity). Disadvantages: Rare groups may end zero rows bootstrap replicates, leading wider undefined CIs. Variance small groups may inflated. Use case examples: Community richness per site habitat. Multi-species indicators (e.g., average occupancy across species). Temporal turnover indicators rely multiple years. Implementation dubicube Default use like tutorials examples.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/whole-cube-versus-group-specific-bootstrap.html","id":"group-specific-bootstrap","dir":"Articles","previous_headings":"Introduction","what":"Group-specific bootstrap","title":"Whole-cube Bootstrap versus Group-specific Bootstrap","text":"Definition: Subset cube group interest (e.g., species year), resample rows within group. Advantages: Guarantees replicate rows group → stable CIs. Reflects within-group variability . Disadvantages: Ignores correlations groups. Variance may slightly underestimated group’s presence correlated groups. Use case examples: Species-specific occupancy habitat preference metrics. Year-specific indicators (e.g., annual richness). Small rare groups zero-row replicates problematic. Implementation dubicube Perform bootstrapping interval calculation per group (e.g. using loop lapply()). See .","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/articles/whole-cube-versus-group-specific-bootstrap.html","id":"an-example-of-a-group-specific-analysis","dir":"Articles","previous_headings":"","what":"An example of a group-specific analysis","title":"Whole-cube Bootstrap versus Group-specific Bootstrap","text":"reuse example introduced bootstrap confidence interval calculation tutorial calculate confidence limits mean number observations per grid cell per year birds Belgium 2011 en 2020 using MGRS grid 10 km scale.","code":"# Load packages library(ggplot2)      # Data visualisation library(dplyr)        # Data wrangling  # Data loading and processing library(frictionless) # Load example datasets library(b3gbi)        # Process occurrence cubes library(dubicube)     # Analysis of data quality & indicator uncertainty"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/whole-cube-versus-group-specific-bootstrap.html","id":"loading-and-processing-the-data","dir":"Articles","previous_headings":"An example of a group-specific analysis","what":"Loading and processing the data","title":"Whole-cube Bootstrap versus Group-specific Bootstrap","text":"load bird cube data b3data data package using frictionless (see also ). process cube b3gbi. First, select 2000 random rows make dataset smaller. reduce computation time tutorial. select data 2011 - 2020.","code":"# Read data package b3data_package <- read_package(   \"https://zenodo.org/records/15211029/files/datapackage.json\" )  # Load bird cube data bird_cube_belgium <- read_resource(b3data_package, \"bird_cube_belgium_mgrs10\") head(bird_cube_belgium) #> # A tibble: 6 × 8 #>    year mgrscode specieskey species          family     n mincoordinateuncerta…¹ #>   <dbl> <chr>         <dbl> <chr>            <chr>  <dbl>                  <dbl> #> 1  2000 31UDS65     2473958 Perdix perdix    Phasi…     1                   3536 #> 2  2000 31UDS65     2474156 Coturnix coturn… Phasi…     1                   3536 #> 3  2000 31UDS65     2474377 Fulica atra      Ralli…     5                   1000 #> 4  2000 31UDS65     2475443 Merops apiaster  Merop…     6                   1000 #> 5  2000 31UDS65     2480242 Vanellus vanell… Chara…     1                   3536 #> 6  2000 31UDS65     2480637 Accipiter nisus  Accip…     1                   3536 #> # ℹ abbreviated name: ¹​mincoordinateuncertaintyinmeters #> # ℹ 1 more variable: familycount <dbl> set.seed(123)  # Make dataset smaller rows <- sample(nrow(bird_cube_belgium), 2000) bird_cube_belgium <- bird_cube_belgium[rows, ]  # Process cube processed_cube <- process_cube(   bird_cube_belgium,   first_year = 2011,   last_year = 2020,   cols_occurrences = \"n\" ) processed_cube #>  #> Processed data cube for calculating biodiversity indicators #>  #> Date Range: 2011 - 2020  #> Single-resolution cube with cell size 10km ^2  #> Number of cells: 242  #> Grid reference system: mgrs  #> Coordinate range: #>    xmin    xmax    ymin    ymax  #>  280000  710000 5490000 5700000  #>  #> Total number of observations: 45143  #> Number of species represented: 253  #> Number of families represented: 57  #>  #> Kingdoms represented: Data not present  #>  #> First 10 rows of data (use n = to show more): #>  #> # A tibble: 957 × 13 #>     year cellCode taxonKey scientificName    family   obs minCoordinateUncerta…¹ #>    <dbl> <chr>       <dbl> <chr>             <chr>  <dbl>                  <dbl> #>  1  2011 31UFS56   5231918 Cuculus canorus   Cucul…    11                   3536 #>  2  2011 31UES28   5739317 Phoenicurus phoe… Musci…     6                   3536 #>  3  2011 31UFS64   6065824 Chroicocephalus … Larid…   143                   1000 #>  4  2011 31UFS96   2492576 Muscicapa striata Musci…     3                   3536 #>  5  2011 31UES04   5231198 Passer montanus   Passe…     1                   3536 #>  6  2011 31UES85   5229493 Garrulus glandar… Corvi…    23                    707 #>  7  2011 31UES88  10124612 Anser anser x Br… Anati…     1                    100 #>  8  2011 31UES22   2481172 Larus marinus     Larid…     8                   1000 #>  9  2011 31UFS43   2481139 Larus argentatus  Larid…    10                   3536 #> 10  2011 31UFT00   9274012 Spatula querqued… Anati…     8                   3536 #> # ℹ 947 more rows #> # ℹ abbreviated name: ¹​minCoordinateUncertaintyInMeters #> # ℹ 6 more variables: familyCount <dbl>, xcoord <dbl>, ycoord <dbl>, #> #   utmzone <int>, hemisphere <chr>, resolution <chr>"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/whole-cube-versus-group-specific-bootstrap.html","id":"analysis-of-the-data","dir":"Articles","previous_headings":"An example of a group-specific analysis","what":"Analysis of the data","title":"Whole-cube Bootstrap versus Group-specific Bootstrap","text":"Let’s say interested mean number observations per grid cell per year. create function calculate . contrast tutorials, now calculate exclusively per year. Therefore, group-specific bootstrap appropriate . indicator (mean observations per year) depends variation within year, correlations across years. get following results: , values don’t reveal much uncertainty surrounds . better understand variability, use bootstrapping estimate distribution yearly means. distribution, can calculate bootstrap confidence intervals.","code":"# Function to calculate statistic of interest # Mean observations per grid cell per year mean_obs <- function(data) {   data %>%     group_by(year, cellCode) %>%     dplyr::mutate(x = mean(obs)) %>%     ungroup() %>%     dplyr::summarise(diversity_val = mean(x), .by = \"year\") %>%     as.data.frame() } mean_obs(processed_cube$data) #>    year diversity_val #> 1  2011      31.28846 #> 2  2012      30.96842 #> 3  2013      36.65049 #> 4  2014      42.17143 #> 5  2015      45.76471 #> 6  2016      39.76068 #> 7  2017     119.70833 #> 8  2018      49.87963 #> 9  2019      21.06780 #> 10 2020      19.78689"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/whole-cube-versus-group-specific-bootstrap.html","id":"group-specific-bootstrapping","dir":"Articles","previous_headings":"An example of a group-specific analysis","what":"Group-specific bootstrapping","title":"Whole-cube Bootstrap versus Group-specific Bootstrap","text":"use bootstrap_cube() function perform bootstrapping (see also bootstrap tutorial). However, now split data per year perform bootstrapping per year using lapply(). store data also output list used interval calculation .","code":"bootstrap_results_group <- processed_cube$data %>%   split(processed_cube$data$year) %>%   lapply(function(cube) {     bootstrap_results <- bootstrap_cube(       data_cube = cube,       fun = mean_obs,       grouping_var = \"year\",       samples = 1000,       seed = 123,       processed_cube = FALSE     )      return(list(bootstrap_results = bootstrap_results, data = cube))   }) head(bootstrap_results_group[[1]]$bootstrap_results) #>   sample year est_original rep_boot est_boot  se_boot bias_boot #> 1      1 2011     31.28846 34.61538 31.40296 5.830663    0.1145 #> 2      2 2011     31.28846 20.95192 31.40296 5.830663    0.1145 #> 3      3 2011     31.28846 20.69231 31.40296 5.830663    0.1145 #> 4      4 2011     31.28846 29.25000 31.40296 5.830663    0.1145 #> 5      5 2011     31.28846 34.47115 31.40296 5.830663    0.1145 #> 6      6 2011     31.28846 32.90385 31.40296 5.830663    0.1145"},{"path":"https://b-cubed-eu.github.io/dubicube/articles/whole-cube-versus-group-specific-bootstrap.html","id":"group-specific-interval-calculation","dir":"Articles","previous_headings":"An example of a group-specific analysis","what":"Group-specific interval calculation","title":"Whole-cube Bootstrap versus Group-specific Bootstrap","text":"Now can use calculate_bootstrap_ci() function calculate confidence limits (see also bootstrap confidence interval calculation tutorial). BCa interval calculation relies jackknifing, also need per group using lapply(). visualise results.","code":"ci_mean_obs_group_list <- bootstrap_results_group %>%   lapply(function(list) {     calculate_bootstrap_ci(       list$bootstrap_results,       grouping_var = \"year\",       type = c(\"perc\", \"bca\", \"norm\", \"basic\"),       conf = 0.95,       data_cube = list$data, # Required for Bca       fun = mean_obs         # Required for Bca     )   })  # Make interval type factor ci_mean_obs_group <- bind_rows(ci_mean_obs_group_list) %>%   mutate(     int_type = factor(       int_type, levels = c(\"perc\", \"bca\", \"norm\", \"basic\")     )   ) head(ci_mean_obs_group) #>   year est_original est_boot  se_boot  bias_boot int_type conf       ll #> 1 2011     31.28846 31.40296 5.830663 0.11450000     perc 0.95 20.83010 #> 2 2011     31.28846 31.40296 5.830663 0.11450000      bca 0.95 22.28181 #> 3 2011     31.28846 31.40296 5.830663 0.11450000     norm 0.95 19.74607 #> 4 2011     31.28846 31.40296 5.830663 0.11450000    basic 0.95 18.74600 #> 5 2012     30.96842 31.02205 4.566094 0.05363158     perc 0.95 22.80027 #> 6 2012     30.96842 31.02205 4.566094 0.05363158      bca 0.95 23.28024 #>         ul #> 1 43.83092 #> 2 46.74330 #> 3 42.60185 #> 4 41.74682 #> 5 40.21998 #> 6 41.32107 ci_mean_obs_group %>%   ggplot(aes(x = year, y = est_original)) +   # Intervals   geom_errorbar(aes(ymin = ll, ymax = ul),                 position = position_dodge(0.8), linewidth = 0.8) +   # Estimates   geom_point(colour = \"firebrick\", size = 2) +   # Settings   labs(y = \"Mean Number of Observations\\nper Grid Cell\") +   scale_x_continuous(breaks = sort(unique(ci_mean_obs_group$year))) +   theme_minimal() +   facet_wrap(~int_type)"},{"path":"https://b-cubed-eu.github.io/dubicube/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Ward Langeraert. Author, maintainer.            Research Institute Nature Forest (INBO) Toon Van Daele. Author.            Research Institute Nature Forest (INBO) Research Institute Nature Forest (INBO). Copyright holder. European Union. Funder.           https://doi.org/10.3030/101059592","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Langeraert, Ward; Van Daele, Toon (2025) dubicube: Calculation Interpretation Data Cube Indicator Uncertainty. Version 0.9.5. https://b-cubed-eu.github.io/dubicube/","code":"@Manual{,   title = {dubicube: Calculation and Interpretation of Data Cube Indicator Uncertainty. Version 0.9.5},   author = {Ward Langeraert and Toon {Van Daele}},   year = {2025},   url = {https://b-cubed-eu.github.io/dubicube/},   abstract = {This R package provides functions to explore data cubes using simple measures and cross-validation techniques. It can also be used for uncertainty calculation using the bootstrap resampling method, and functionality is provided for efficient interpretation and visualisation of uncertainty related to indicators based on occurrence cubes.},   keywords = {uncertainty quantification; uncertainty visualisation; biodiversity indicators; data cubes},   doi = {10.5281/zenodo.14850237}, }"},{"path":"https://b-cubed-eu.github.io/dubicube/index.html","id":"dubicube-","dir":"","previous_headings":"","what":"Calculation and Interpretation of Data Cube Indicator Uncertainty","title":"Calculation and Interpretation of Data Cube Indicator Uncertainty","text":"dubicube package aims deliver measures assessing applicability biodiversity data cubes, whether general use specific biodiversity indicators. measures facilitate data exploration providing insights data quality reliability. Additionally, package includes functions calculating indicator uncertainty using bootstrapping, well tools interpreting visualising uncertainty biodiversity indicators derived occurrence cubes.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Calculation and Interpretation of Data Cube Indicator Uncertainty","text":"Install dubicube R: can install development version GitHub :","code":"install.packages(\"dubicube\", repos = c(\"https://b-cubed-eu.r-universe.dev\", \"https://cloud.r-project.org\")) # install.packages(\"remotes\") remotes::install_github(\"b-cubed-eu/dubicube\")"},{"path":"https://b-cubed-eu.github.io/dubicube/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key Features","title":"Calculation and Interpretation of Data Cube Indicator Uncertainty","text":"dubicube package offers:","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/index.html","id":"mag-1-data-exploration--variability-assessment","dir":"","previous_headings":"Key Features","what":"🔍 1. Data Exploration & Variability Assessment","title":"Calculation and Interpretation of Data Cube Indicator Uncertainty","text":"Gain insights structure sensitivity biodiversity data cubes. cross_validate_cube() Perform cross-validation (leave-one-k-fold) assess group-level sensitivity indicators evaluate individual categories influence results. 📘 Read tutorial → 🛠️ Additional data quality diagnostics way!","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/index.html","id":"chart_with_upwards_trend-2-estimating-indicator-uncertainty","dir":"","previous_headings":"Key Features","what":"📈 2. Estimating Indicator Uncertainty","title":"Calculation and Interpretation of Data Cube Indicator Uncertainty","text":"Use bootstrap methods understand variability, bias, confidence indicators. bootstrap_cube() Create bootstrap replicates estimate indicator variability, bias, standard error. 📘 Read tutorial → calculate_bootstrap_ci() Compute confidence intervals (percentile, BCa, normal, basic), optional transformations bias correction. 📘 Read tutorial →","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/index.html","id":"brain-3-interpretation--visualisation","dir":"","previous_headings":"Key Features","what":"🧠 3. Interpretation & Visualisation","title":"Calculation and Interpretation of Data Cube Indicator Uncertainty","text":"Put results context reference values uncertainty thresholds. add_effect_classification() Classify indicator trends (e.g. increase, stable, decrease) comparing confidence intervals thresholds. 📘 Read tutorial → Spatial temporal interpretation Learn visualise assess patterns across space time using indicator uncertainty. 📘 Best practices temporal trends → 📘 Best practices spatial trends → 🔗 Learn website explore documentation.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/add_effect_classification.html","id":null,"dir":"Reference","previous_headings":"","what":"Add effect classifications to a dataframe by comparing the confidence intervals with a reference and thresholds — add_effect_classification","title":"Add effect classifications to a dataframe by comparing the confidence intervals with a reference and thresholds — add_effect_classification","text":"function adds classified effects dataframe ordered factor variables comparing confidence intervals reference thresholds.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/add_effect_classification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add effect classifications to a dataframe by comparing the confidence intervals with a reference and thresholds — add_effect_classification","text":"","code":"add_effect_classification(   df,   cl_columns,   threshold,   reference = 0,   coarse = TRUE )"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/add_effect_classification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add effect classifications to a dataframe by comparing the confidence intervals with a reference and thresholds — add_effect_classification","text":"df dataframe containing summary data confidence limits. Two columns required containing lower upper limits indicated cl_columns argument. columns optional. cl_columns vector 2 column names df indicating respectively lower upper confidence limits (e.g. c(\"lcl\", \"ucl\")). threshold vector either 1 2 thresholds. single threshold transformed reference + c(-abs(threshold), abs(threshold)). reference null hypothesis value compare confidence intervals . Defaults 0. coarse Logical, defaults TRUE. TRUE, add coarse classification dataframe.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/add_effect_classification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add effect classifications to a dataframe by comparing the confidence intervals with a reference and thresholds — add_effect_classification","text":"returned value modified version original input dataframe df additional columns effect_code effect containing respectively effect symbols descriptions ordered factor variables. case coarse = TRUE (default) also effect_code_coarse effect_coarse containing coarse classification effects.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/add_effect_classification.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add effect classifications to a dataframe by comparing the confidence intervals with a reference and thresholds — add_effect_classification","text":"function wrapper around effectclass::classify() effectclass::coarse_classification() effectclass package (Onkelinx, 2023). classify effects stable transparent manner.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/add_effect_classification.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Add effect classifications to a dataframe by comparing the confidence intervals with a reference and thresholds — add_effect_classification","text":"Onkelinx, T. (2023). effectclass: Classification visualisation effects [Computer software]. https://inbo.github.io/effectclass/","code":""},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/reference/add_effect_classification.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add effect classifications to a dataframe by comparing the confidence intervals with a reference and thresholds — add_effect_classification","text":"","code":"# Example dataset ds <- data.frame(   mean = c(0, 0.5, -0.5, 1, -1, 1.5, -1.5, 0.5, -0.5, 0),   sd = c(1, 0.5, 0.5, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25, 0.5) ) ds$lcl <- qnorm(0.05, ds$mean, ds$sd) ds$ucl <- qnorm(0.95, ds$mean, ds$sd)  add_effect_classification(  df = ds,  cl_columns = c(\"lcl\", \"ucl\"),  threshold = 1,  reference = 0,  coarse = TRUE ) #>    mean   sd         lcl         ucl effect_code effect_code_coarse #> 1   0.0 1.00 -1.64485363  1.64485363           ?                  ? #> 2   0.5 0.50 -0.32242681  1.32242681          ?+                  ? #> 3  -0.5 0.50 -1.32242681  0.32242681          ?-                  ? #> 4   1.0 0.50  0.17757319  1.82242681           +                  + #> 5  -1.0 0.50 -1.82242681 -0.17757319           -                  - #> 6   1.5 0.25  1.08878659  1.91121341          ++                  + #> 7  -1.5 0.25 -1.91121341 -1.08878659          --                  - #> 8   0.5 0.25  0.08878659  0.91121341          +~                  + #> 9  -0.5 0.25 -0.91121341 -0.08878659          -~                  - #> 10  0.0 0.50 -0.82242681  0.82242681           ~                  ~ #>                effect effect_coarse #> 1             unknown       unknown #> 2  potential increase       unknown #> 3  potential decrease       unknown #> 4            increase      increase #> 5            decrease      decrease #> 6     strong increase      increase #> 7     strong decrease      decrease #> 8   moderate increase      increase #> 9   moderate decrease      decrease #> 10             stable        stable"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/basic_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate basic bootstrap confidence interval — basic_ci","title":"Calculate basic bootstrap confidence interval — basic_ci","text":"function calculates basic confidence interval bootstrap sample. used calculate_bootstrap_ci().","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/basic_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate basic bootstrap confidence interval — basic_ci","text":"","code":"basic_ci(t0, t, conf = 0.95, h = function(t) t, hinv = function(t) t)"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/basic_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate basic bootstrap confidence interval — basic_ci","text":"t0 Original statistic. t Numeric vector bootstrap replicates. conf numeric value specifying confidence level interval. Default 0.95 (95 % confidence level). h function defining transformation. intervals calculated scale h(t) inverse function hinv applied resulting intervals. must function one variable . default identity function. hinv function, like h, returns inverse h. used transform intervals calculated scale h(t) back original scale. default identity function. h supplied hinv , intervals returned transformed scale.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/basic_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate basic bootstrap confidence interval — basic_ci","text":"matrix four columns: conf: confidence level rk_lower: rank lower endpoint (interpolated) rk_upper: rank upper endpoint (interpolated) ll: lower confidence limit ul: lower confidence limit","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/basic_ci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate basic bootstrap confidence interval — basic_ci","text":"$$CI_{basic} = \\left[ 2\\hat{\\theta} - \\hat{\\theta}^*_{(1-\\alpha/2)}, 2\\hat{\\theta} - \\hat{\\theta}^*_{(\\alpha/2)} \\right]$$ \\(\\hat{\\theta}^*_{(\\alpha/2)}\\) \\(\\hat{\\theta}^*_{(1-\\alpha/2)}\\) \\(\\alpha/2\\) \\(1-\\alpha/2\\) percentiles bootstrap distribution, respectively.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/basic_ci.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Calculate basic bootstrap confidence interval — basic_ci","text":"function adapted internal function basic.ci() boot package (Canty & Ripley, 1999).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/basic_ci.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate basic bootstrap confidence interval — basic_ci","text":"Canty, ., & Ripley, B. (1999). boot: Bootstrap Functions (Originally Angelo Canty S) [Computer software]. https://CRAN.R-project.org/package=boot Davison, . C., & Hinkley, D. V. (1997). Bootstrap Methods Application (1st ed.). Cambridge University Press. doi:10.1017/CBO9780511802843","code":""},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/reference/basic_ci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate basic bootstrap confidence interval — basic_ci","text":"","code":"set.seed(123) boot_reps <- rnorm(1000) t0 <- mean(boot_reps)  # Basic bootstrap CI basic_ci(t0, boot_reps, conf = 0.95) #>      conf rk_lower rk_upper        ll       ul #> [1,] 0.95    25.03   975.98 -2.017511 1.975176"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bca_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Bias-Corrected and Accelerated (BCa) bootstrap confidence interval — bca_ci","title":"Calculate Bias-Corrected and Accelerated (BCa) bootstrap confidence interval — bca_ci","text":"function calculates Bias-Corrected Accelerated (BCa) confidence interval bootstrap sample. used calculate_bootstrap_ci().","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bca_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Bias-Corrected and Accelerated (BCa) bootstrap confidence interval — bca_ci","text":"","code":"bca_ci(t0, t, a, conf = 0.95, h = function(t) t, hinv = function(t) t)"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bca_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Bias-Corrected and Accelerated (BCa) bootstrap confidence interval — bca_ci","text":"t0 Original statistic. t Numeric vector bootstrap replicates. Acceleration constant. See also calculate_acceleration(). conf numeric value specifying confidence level interval. Default 0.95 (95 % confidence level). h function defining transformation. intervals calculated scale h(t) inverse function hinv applied resulting intervals. must function one variable . default identity function. hinv function, like h, returns inverse h. used transform intervals calculated scale h(t) back original scale. default identity function. h supplied hinv , intervals returned transformed scale.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bca_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Bias-Corrected and Accelerated (BCa) bootstrap confidence interval — bca_ci","text":"matrix four columns: conf: confidence level rk_lower: rank lower endpoint (interpolated) rk_upper: rank upper endpoint (interpolated) ll: lower confidence limit ul: lower confidence limit","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bca_ci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Bias-Corrected and Accelerated (BCa) bootstrap confidence interval — bca_ci","text":"Adjusts bias acceleration. Bias refers systematic difference observed statistic original dataset center bootstrap distribution statistic. bias correction term calculated follows: $$\\hat{z}_0 = \\Phi^{-1}\\left(\\frac{\\#(\\hat{\\theta}^*_b < \\hat{\\theta})}{B}\\right)$$ \\(\\#\\) counting operator, counting number times \\(\\hat{\\theta}^*_b\\) smaller \\(\\hat{\\theta}\\), \\(\\Phi^{-1}\\) inverse cumulative density function standard normal distribution.\\(B\\) number bootstrap samples. Acceleration quantifies sensitive variability statistic changes data. See calculate_acceleration() calculated. \\(=0\\): statistic's variability depend data (e.g., symmetric distribution) \\(>0\\): Small changes data large effect statistic's variability (e.g., positive skew) \\(<0\\): Small changes data smaller effect statistic's variability (e.g., negative skew). bias acceleration estimates used calculate adjusted percentiles. \\(\\alpha_1 = \\Phi\\left( \\hat{z}_0 + \\frac{\\hat{z}_0 + z_{\\alpha/2}}{1 - \\hat{}(\\hat{z}_0 + z_{\\alpha/2})} \\right)\\), \\(\\alpha_2 = \\Phi\\left( \\hat{z}_0 + \\frac{\\hat{z}_0 + z_{1 - \\alpha/2}}{1 - \\hat{}(\\hat{z}_0 + z_{1 - \\alpha/2})} \\right)\\) , get $$CI_{bca} = \\left[ \\hat{\\theta}^*_{(\\alpha_1)}, \\hat{\\theta}^*_{(\\alpha_2)} \\right]$$","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bca_ci.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Calculate Bias-Corrected and Accelerated (BCa) bootstrap confidence interval — bca_ci","text":"function adapted internal function bca.ci() boot package (Canty & Ripley, 1999).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bca_ci.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate Bias-Corrected and Accelerated (BCa) bootstrap confidence interval — bca_ci","text":"Canty, ., & Ripley, B. (1999). boot: Bootstrap Functions (Originally Angelo Canty S) [Computer software]. https://CRAN.R-project.org/package=boot Davison, . C., & Hinkley, D. V. (1997). Bootstrap Methods Application (1st ed.). Cambridge University Press. doi:10.1017/CBO9780511802843","code":""},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bca_ci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Bias-Corrected and Accelerated (BCa) bootstrap confidence interval — bca_ci","text":"","code":"set.seed(123) boot_reps <- rnorm(1000) t0 <- mean(boot_reps)  # Example acceleration value (normally estimated via jackknife) a <- 0.01  # BCa bootstrap CI bca_ci(t0, boot_reps, a, conf = 0.95) #>      conf rk_lower rk_upper        ll       ul #> [1,] 0.95    27.62   978.46 -1.930133 2.112914"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bootstrap_cube.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform bootstrapping over a data cube for a calculated statistic — bootstrap_cube","title":"Perform bootstrapping over a data cube for a calculated statistic — bootstrap_cube","text":"function generate samples bootstrap replicates statistic applied data cube. resamples data cube computes statistic fun bootstrap replicate, optionally comparing results reference group (ref_group).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bootstrap_cube.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform bootstrapping over a data cube for a calculated statistic — bootstrap_cube","text":"","code":"bootstrap_cube(   data_cube,   fun,   ...,   grouping_var,   samples = 1000,   ref_group = NA,   seed = NA,   processed_cube = TRUE,   progress = FALSE )"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bootstrap_cube.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform bootstrapping over a data cube for a calculated statistic — bootstrap_cube","text":"data_cube data cube object (class 'processed_cube' 'sim_cube', see b3gbi::process_cube()) dataframe (cf. $data slot 'processed_cube' 'sim_cube'). processed_cube = TRUE (default), must processed simulated data cube contains $data element. fun function , applied data_cube$data returns statistic(s) interest (just data_cube case dataframe). function must return dataframe column diversity_val containing statistic interest. ... Additional arguments passed fun. grouping_var character vector specifying grouping variable(s) bootstrap analysis. function fun(data_cube$data, ...) return row per group. specified variables must redundant, meaning contain information (e.g., \"time_point\" (1, 2, 3) \"year\" (2000, 2001, 2002) used together \"time_point\" just alternative encoding \"year\"). samples number bootstrap replicates. single positive integer. Default 1000. ref_group string indicating reference group compare statistic . Default NA, meaning reference group used. seed positive numeric value setting seed random number generation ensure reproducibility. NA (default), set.seed() called . NA, random number generator state reset (state calling function) upon exiting function. processed_cube Logical. TRUE (default), function expects data_cube data cube object $data slot. FALSE, function expects data_cube dataframe. progress Logical. Whether show progress bar. Set TRUE display progress bar, FALSE (default) suppress .","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bootstrap_cube.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform bootstrapping over a data cube for a calculated statistic — bootstrap_cube","text":"dataframe containing bootstrap results following columns: sample: Sample ID bootstrap replicate est_original: statistic based full dataset per group rep_boot: statistic based bootstrapped dataset (bootstrap replicate) est_boot: bootstrap estimate (mean bootstrap replicates per group) se_boot: standard error bootstrap estimate (standard deviation bootstrap replicates per group) bias_boot: bias bootstrap estimate per group","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bootstrap_cube.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Perform bootstrapping over a data cube for a calculated statistic — bootstrap_cube","text":"Bootstrapping statistical technique used estimate distribution statistic resampling replacement original data (Davison & Hinkley, 1997; Efron & Tibshirani, 1994). case data cubes, row sampled replacement. common notations used bootstrapping: Original Sample Data: \\(\\mathbf{X} = \\{X_1, X_2, \\ldots, X_n\\}\\) initial set data points. , \\(n\\) sample size. corresponds number cells data cube number rows tabular format. Statistic Interest: \\(\\theta\\) parameter statistic estimated, mean \\(\\bar{X}\\), variance \\(\\sigma^2\\), biodiversity indicator. Let \\(\\hat{\\theta}\\) denote estimated value \\(\\theta\\) calculated complete dataset \\(\\mathbf{X}\\). Bootstrap Sample: \\(\\mathbf{X}^* = \\{X_1^*, X_2^*, \\ldots, X_n^*\\}\\) sample size \\(n\\) drawn replacement original sample \\(\\mathbf{X}\\). \\(X_i^*\\) drawn independently \\(\\mathbf{X}\\). total \\(B\\) bootstrap samples drawn original data. Common choices \\(B\\) 1000 10,000 ensure good approximation distribution bootstrap replications (see ). Bootstrap Replication: \\(\\hat{\\theta}^*_b\\) value statistic interest calculated \\(b\\)-th bootstrap sample \\(\\mathbf{X}^*_b\\). example, \\(\\theta\\) sample mean, \\(\\hat{\\theta}^*_b = \\bar{X}^*_b\\). Bootstrap Statistics: Bootstrap Estimate Statistic: \\(\\hat{\\theta}_{\\text{boot}}\\) average bootstrap replications: $$\\hat{\\theta}_{\\text{boot}} = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*_b$$ Bootstrap Bias: \\(\\text{Bias}_{\\text{boot}}\\) bias indicates much bootstrap estimate deviates original sample estimate. calculated difference average bootstrap estimate original estimate: $$\\text{Bias}_{\\text{boot}} = \\frac{1}{B} \\sum_{b=1}^B (\\hat{\\theta}^*_b - \\hat{\\theta}) = \\hat{\\theta}_{\\text{boot}} - \\hat{\\theta}$$ Bootstrap Standard Error: \\(\\text{SE}_{\\text{boot}}\\) standard deviation bootstrap replications, estimates variability statistic.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bootstrap_cube.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Perform bootstrapping over a data cube for a calculated statistic — bootstrap_cube","text":"Davison, . C., & Hinkley, D. V. (1997). Bootstrap Methods Application (1st ed.). Cambridge University Press. doi:10.1017/CBO9780511802843 Efron, B., & Tibshirani, R. J. (1994). Introduction Bootstrap (1st ed.). Chapman Hall/CRC. doi:10.1201/9780429246593","code":""},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/reference/bootstrap_cube.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform bootstrapping over a data cube for a calculated statistic — bootstrap_cube","text":"","code":"if (FALSE) { # \\dontrun{ # After processing a data cube with b3gbi::process_cube()  # Function to calculate statistic of interest # Mean observations per year mean_obs <- function(data) {   out_df <- aggregate(obs ~ year, data, mean) # Calculate mean obs per year   names(out_df) <- c(\"year\", \"diversity_val\") # Rename columns   return(out_df) } mean_obs(processed_cube$data)  # Perform bootstrapping bootstrap_mean_obs <- bootstrap_cube(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   samples = 1000,   seed = 123,   progress = FALSE ) head(bootstrap_mean_obs) } # }"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_acceleration.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate acceleration for a statistic in a dataframe — calculate_acceleration","title":"Calculate acceleration for a statistic in a dataframe — calculate_acceleration","text":"function calculates acceleration values, quantify sensitivity statistic’s variability changes dataset. Acceleration used bias-corrected accelerated (BCa) confidence intervals calculate_bootstrap_ci().","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_acceleration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate acceleration for a statistic in a dataframe — calculate_acceleration","text":"","code":"calculate_acceleration(   data_cube,   fun,   ...,   grouping_var,   ref_group = NA,   influence_method = \"usual\",   processed_cube = TRUE,   progress = FALSE )"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_acceleration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate acceleration for a statistic in a dataframe — calculate_acceleration","text":"data_cube data cube object (class 'processed_cube' 'sim_cube', see b3gbi::process_cube()) dataframe (cf. $data slot 'processed_cube' 'sim_cube'). processed_cube = TRUE (default), must processed simulated data cube contains $data element. fun function , applied data_cube$data returns statistic(s) interest (just data_cube case dataframe). function must return dataframe column diversity_val containing statistic interest. used bootstrap_cube(). ... Additional arguments passed fun. grouping_var character vector specifying grouping variable(s) bootstrap analysis. function fun(data_cube$data, ...) return row per group. specified variables must redundant, meaning contain information (e.g., \"time_point\" (1, 2, 3) \"year\" (2000, 2001, 2002) used together \"time_point\" just alternative encoding \"year\"). variable used split dataset groups separate acceleration calculations. ref_group string indicating reference group compare statistic . Default NA, meaning reference group used. used bootstrap_cube(). influence_method string specifying method used calculating influence values. \"usual\": Negative jackknife (default BCa selected). \"pos\": Positive jackknife processed_cube Logical. TRUE (default), function expects data_cube data cube object $data slot. FALSE, function expects data_cube dataframe. progress Logical. Whether show progress bar jackknifing. Set TRUE display progress bar, FALSE (default) suppress .","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_acceleration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate acceleration for a statistic in a dataframe — calculate_acceleration","text":"dataframe containing acceleration values per grouping_var.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_acceleration.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate acceleration for a statistic in a dataframe — calculate_acceleration","text":"Acceleration quantifies sensitive variability statistic \\(\\theta\\) changes data. \\(=0\\): statistic's variability depend data (e.g., symmetric distribution) \\(>0\\): Small changes data large effect statistic's variability (e.g., positive skew) \\(<0\\): Small changes data smaller effect statistic's variability (e.g., negative skew). used BCa confidence interval calculation, adjust bias skewness bootstrapped distributions (Davison & Hinkley, 1997, Chapter 5). See also empinf() function boot package R (Canty & Ripley, 1999)). acceleration calculated follows: $$\\hat{} = \\frac{1}{6} \\frac{\\sum_{= 1}^{n}(I_i^3)}{\\left( \\sum_{= 1}^{n}(I_i^2) \\right)^{3/2}}$$ \\(I_i\\) denotes influence data point \\(x_i\\) estimation \\(\\theta\\). \\(I_i\\) can estimated using jackknifing. Examples (1) negative jackknife: \\(I_i = (n-1)(\\hat{\\theta} - \\hat{\\theta}_{-})\\), (2) positive jackknife \\(I_i = (n+1)(\\hat{\\theta}_{-} - \\hat{\\theta})\\) (Frangos & Schucany, 1990). , \\(\\hat{\\theta}_{-}\\) estimated value leaving \\(\\)’th data point \\(x_i\\). boot package also offers infinitesimal jackknife regression estimation. Implementation jackknife algorithms can explored future. reference group used, jackknifing implemented different way. Consider \\(\\hat{\\theta} = \\hat{\\theta}_1 - \\hat{\\theta}_2\\) \\(\\hat{\\theta}_1\\) estimate indicator value non-reference period (sample size \\(n_1\\)) \\(\\hat{\\theta}_2\\) estimate indicator value reference period (sample size \\(n_2\\)). acceleration now calculated follows: $$\\hat{} = \\frac{1}{6} \\frac{\\sum_{= 1}^{n_1 + n_2}(I_i^3)}{\\left( \\sum_{= 1}^{n_1 + n_2}(I_i^2) \\right)^{3/2}}$$ \\(I_i\\) can calculated using negative positive jackknife. \\(\\hat{\\theta}_{-} = \\hat{\\theta}_{1,-} - \\hat{\\theta}_2 \\text{ } = 1, \\ldots, n_1\\), \\(\\hat{\\theta}_{-} = \\hat{\\theta}_{1} - \\hat{\\theta}_{2,-} \\text{ } = n_1 + 1, \\ldots, n_1 + n_2\\)","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_acceleration.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate acceleration for a statistic in a dataframe — calculate_acceleration","text":"Canty, ., & Ripley, B. (1999). boot: Bootstrap Functions (Originally Angelo Canty S) [Computer software]. https://CRAN.R-project.org/package=boot Davison, . C., & Hinkley, D. V. (1997). Bootstrap Methods Application (1st ed.). Cambridge University Press. doi:10.1017/CBO9780511802843 Frangos, C. C., & Schucany, W. R. (1990). Jackknife estimation bootstrap acceleration constant. Computational Statistics & Data Analysis, 9(3), 271–281. doi:10.1016/0167-9473(90)90109-U","code":""},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_acceleration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate acceleration for a statistic in a dataframe — calculate_acceleration","text":"","code":"if (FALSE) { # \\dontrun{ # After processing a data cube with b3gbi::process_cube()  # Function to calculate statistic of interest # Mean observations per year mean_obs <- function(data) {   out_df <- aggregate(obs ~ year, data, mean) # Calculate mean obs per year   names(out_df) <- c(\"year\", \"diversity_val\") # Rename columns   return(out_df) } mean_obs(processed_cube$data)  # Calculate acceleration acceleration_df <- calculate_acceleration(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   progress = FALSE ) acceleration_df } # }"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_bootstrap_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate confidence intervals for a dataframe with bootstrap replicates — calculate_bootstrap_ci","title":"Calculate confidence intervals for a dataframe with bootstrap replicates — calculate_bootstrap_ci","text":"function calculates confidence intervals dataframe containing bootstrap replicates based different methods, including percentile (perc), bias-corrected accelerated (bca), normal (norm), basic (basic).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_bootstrap_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate confidence intervals for a dataframe with bootstrap replicates — calculate_bootstrap_ci","text":"","code":"calculate_bootstrap_ci(   bootstrap_samples_df,   grouping_var,   type = c(\"perc\", \"bca\", \"norm\", \"basic\"),   conf = 0.95,   h = function(t) t,   hinv = function(t) t,   no_bias = FALSE,   aggregate = TRUE,   data_cube = NA,   fun = NA,   ...,   ref_group = NA,   influence_method = ifelse(is.element(\"bca\", type), \"usual\", NA),   progress = FALSE )"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_bootstrap_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate confidence intervals for a dataframe with bootstrap replicates — calculate_bootstrap_ci","text":"bootstrap_samples_df dataframe containing bootstrap replicates, row represents bootstrap sample. returned bootstrap_cube(). Apart grouping_var column, following columns present: est_original: statistic based full dataset per group rep_boot: statistic based bootstrapped dataset (bootstrap replicate) grouping_var character vector specifying grouping variable(s) bootstrap analysis. function fun(data_cube$data, ...) return row per group. specified variables must redundant, meaning contain information (e.g., \"time_point\" (1, 2, 3) \"year\" (2000, 2001, 2002) used together \"time_point\" just alternative encoding \"year\"). variable used split dataset groups separate confidence interval calculations. type character vector specifying type(s) confidence intervals compute. Options include: \"perc\": Percentile interval \"bca\": Bias-corrected accelerated interval \"norm\": Normal interval \"basic\": Basic interval \"\": Compute available interval types (default) conf numeric value specifying confidence level intervals. Default 0.95 (95 % confidence level). h function defining transformation. intervals calculated scale h(t) inverse function hinv applied resulting intervals. must function one variable . default identity function. hinv function, like h, returns inverse h. used transform intervals calculated scale h(t) back original scale. default identity function. h supplied hinv , intervals returned transformed scale. no_bias Logical. TRUE intervals centered around original estimates (bias ignored). Default FALSE. aggregate Logical. TRUE (default), function returns distinct confidence limits per group. FALSE, confidence limits added original bootstrap dataframe bootstrap_samples_df. data_cube used type = \"bca\". data cube object (class 'processed_cube' 'sim_cube', see b3gbi::process_cube()) dataframe (cf. $data slot 'processed_cube' 'sim_cube'). used bootstrap_cube(). fun used type = \"bca\". function , applied data_cube$data returns statistic(s) interest (just data_cube case dataframe). function must return dataframe column diversity_val containing statistic interest. used bootstrap_cube(). ... Additional arguments passed fun. ref_group used type = \"bca\". string indicating reference group compare statistic . Default NA, meaning reference group used. used bootstrap_cube(). influence_method string specifying method used calculating influence values. \"usual\": Negative jackknife (default BCa selected). \"pos\": Positive jackknife progress Logical. Whether show progress bar jackknifing. Set TRUE display progress bar, FALSE (default) suppress .","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_bootstrap_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate confidence intervals for a dataframe with bootstrap replicates — calculate_bootstrap_ci","text":"dataframe containing bootstrap results following columns: est_original: statistic based full dataset per group rep_boo est_boot: bootstrap estimate (mean bootstrap replicates per group) se_boot: standard error bootstrap estimate (standard deviation bootstrap replicates per group) bias_boot: bias bootstrap estimate per group int_type: interval type ll: lower limit confidence interval ul: upper limit confidence interval conf: confidence level interval aggregate = FALSE, dataframe contains columns bootstrap_samples_df one row per bootstrap replicate.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_bootstrap_ci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate confidence intervals for a dataframe with bootstrap replicates — calculate_bootstrap_ci","text":"consider four different types intervals (confidence level \\(\\alpha\\)). choice confidence interval types calculation line boot package R (Canty & Ripley, 1999) ensure ease implementation. based definitions provided Davison & Hinkley (1997, Chapter 5) (see also DiCiccio & Efron, 1996; Efron, 1987). Percentile: Uses percentiles bootstrap distribution. $$CI_{perc} = \\left[ \\hat{\\theta}^*_{(\\alpha/2)}, \\hat{\\theta}^*_{(1-\\alpha/2)} \\right]$$ \\(\\hat{\\theta}^*_{(\\alpha/2)}\\) \\(\\hat{\\theta}^*_{(1-\\alpha/2)}\\) \\(\\alpha/2\\) \\(1-\\alpha/2\\) percentiles bootstrap distribution, respectively. Bias-Corrected Accelerated (BCa): Adjusts bias acceleration Bias refers systematic difference observed statistic original dataset center bootstrap distribution statistic. bias correction term calculated follows: $$\\hat{z}_0 = \\Phi^{-1}\\left(\\frac{\\#(\\hat{\\theta}^*_b < \\hat{\\theta})}{B}\\right)$$ \\(\\#\\) counting operator, counting number times \\(\\hat{\\theta}^*_b\\) smaller \\(\\hat{\\theta}\\), \\(\\Phi^{-1}\\) inverse cumulative density function standard normal distribution.\\(B\\) number bootstrap samples. Acceleration quantifies sensitive variability statistic changes data. See calculate_acceleration() calculated. \\(=0\\): statistic's variability depend data (e.g., symmetric distribution) \\(>0\\): Small changes data large effect statistic's variability (e.g., positive skew) \\(<0\\): Small changes data smaller effect statistic's variability (e.g., negative skew). bias acceleration estimates used calculate adjusted percentiles. \\(\\alpha_1 = \\Phi\\left( \\hat{z}_0 + \\frac{\\hat{z}_0 + z_{\\alpha/2}}{1 - \\hat{}(\\hat{z}_0 + z_{\\alpha/2})} \\right)\\), \\(\\alpha_2 = \\Phi\\left( \\hat{z}_0 + \\frac{\\hat{z}_0 + z_{1 - \\alpha/2}}{1 - \\hat{}(\\hat{z}_0 + z_{1 - \\alpha/2})} \\right)\\) , get $$CI_{bca} = \\left[ \\hat{\\theta}^*_{(\\alpha_1)}, \\hat{\\theta}^*_{(\\alpha_2)} \\right]$$ Normal: Assumes bootstrap distribution statistic approximately normal $$CI_{norm} = \\left[\\hat{\\theta} - \\text{Bias}_{\\text{boot}} - \\text{SE}_{\\text{boot}} \\times z_{1-\\alpha/2},    \\hat{\\theta} - \\text{Bias}_{\\text{boot}} + \\text{SE}_{\\text{boot}} \\times z_{1-\\alpha/2} \\right]$$ \\(z_{1-\\alpha/2}\\) \\(1-\\alpha/2\\) quantile standard normal distribution. Basic: Centers interval using percentiles $$CI_{basic} = \\left[ 2\\hat{\\theta} - \\hat{\\theta}^*_{(1-\\alpha/2)},    2\\hat{\\theta} - \\hat{\\theta}^*_{(\\alpha/2)} \\right]$$ \\(\\hat{\\theta}^*_{(\\alpha/2)}\\) \\(\\hat{\\theta}^*_{(1-\\alpha/2)}\\) \\(\\alpha/2\\) \\(1-\\alpha/2\\) percentiles bootstrap distribution, respectively.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_bootstrap_ci.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate confidence intervals for a dataframe with bootstrap replicates — calculate_bootstrap_ci","text":"Canty, ., & Ripley, B. (1999). boot: Bootstrap Functions (Originally Angelo Canty S) [Computer software]. https://CRAN.R-project.org/package=boot Davison, . C., & Hinkley, D. V. (1997). Bootstrap Methods Application (1st ed.). Cambridge University Press. doi:10.1017/CBO9780511802843 DiCiccio, T. J., & Efron, B. (1996). Bootstrap confidence intervals. Statistical Science, 11(3). doi:10.1214/ss/1032280214 Efron, B. (1987). Better Bootstrap Confidence Intervals. Journal American Statistical Association, 82(397), 171–185. doi:10.1080/01621459.1987.10478410 Efron, B., & Tibshirani, R. J. (1994). Introduction Bootstrap (1st ed.). Chapman Hall/CRC. doi:10.1201/9780429246593","code":""},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/reference/calculate_bootstrap_ci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate confidence intervals for a dataframe with bootstrap replicates — calculate_bootstrap_ci","text":"","code":"if (FALSE) { # \\dontrun{ # After processing a data cube with b3gbi::process_cube()  # Function to calculate statistic of interest # Mean observations per year mean_obs <- function(data) {   out_df <- aggregate(obs ~ year, data, mean) # Calculate mean obs per year   names(out_df) <- c(\"year\", \"diversity_val\") # Rename columns   return(out_df) } mean_obs(processed_cube$data)  # Perform bootstrapping bootstrap_mean_obs <- bootstrap_cube(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   samples = 1000,   seed = 123,   progress = FALSE ) head(bootstrap_mean_obs)  # Calculate confidence limits # Percentile interval ci_mean_obs1 <- calculate_bootstrap_ci(   bootstrap_samples_df = bootstrap_mean_obs,   grouping_var = \"year\",   type = \"perc\",   conf = 0.95,   aggregate = TRUE ) ci_mean_obs1  # All intervals ci_mean_obs2 <- calculate_bootstrap_ci(   bootstrap_samples_df = bootstrap_mean_obs,   grouping_var = \"year\",   type = c(\"perc\", \"bca\", \"norm\", \"basic\"),   conf = 0.95,   aggregate = TRUE,   data_cube = processed_cube, # Required for BCa   fun = mean_obs,             # Required for BCa   progress = FALSE ) ci_mean_obs2 } # }"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/cross_validate_cube.html","id":null,"dir":"Reference","previous_headings":"","what":"Leave-one-out cross-validation for data cubes — cross_validate_cube","title":"Leave-one-out cross-validation for data cubes — cross_validate_cube","text":"function performs leave-one-(LOO) k-fold (experimental) cross-validation (CV) biodiversity data cube assess performance specified indicator function. partitions data specified variable, calculates specified indicator training data, compares true values evaluate influence one categories final result.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/cross_validate_cube.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-one-out cross-validation for data cubes — cross_validate_cube","text":"","code":"cross_validate_cube(   data_cube,   fun,   ...,   grouping_var,   out_var = \"taxonKey\",   crossv_method = c(\"loo\", \"kfold\"),   k = ifelse(crossv_method == \"kfold\", 5, NA),   max_out_cats = 1000,   processed_cube = TRUE,   progress = FALSE )"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/cross_validate_cube.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-one-out cross-validation for data cubes — cross_validate_cube","text":"data_cube data cube object (class 'processed_cube' 'sim_cube', see b3gbi::process_cube()) dataframe (cf. $data slot 'processed_cube' 'sim_cube'). processed_cube = TRUE (default), must processed simulated data cube contains $data element. fun function , applied data_cube$data returns statistic(s) interest (just data_cube case dataframe). function must return dataframe column diversity_val containing statistic interest. ... Additional arguments passed fun. grouping_var character vector specifying grouping variable(s) fun. output fun(data_cube) returns row per group. out_var string specifying column data left iteratively. Default \"taxonKey\" can used leave-one-species-CV. crossv_method Method data partitioning. crossv_method = \"loo\" (default), S = number unique values out_var training partitions created containing S - 1 rows . crossv_method = \"kfold\", aggregated data split data k exclusive partitions containing S / k rows . K-fold CV experimental results interpreted caution. k Number folds (integer). Used crossv_method = \"kfold\". Default 5. max_out_cats integer specifying maximum number unique categories out_var leave iteratively. Default 1000. can increased needed, keep mind high number categories out_var may significantly increase runtime. processed_cube Logical. TRUE (default), function expects data_cube data cube object $data slot. FALSE, function expects data_cube dataframe. progress Logical. Whether show progress bar. Set TRUE display progress bar, FALSE (default) suppress .","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/cross_validate_cube.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Leave-one-out cross-validation for data cubes — cross_validate_cube","text":"dataframe containing cross-validation results following columns: Cross-Validation id (id_cv) grouping variable grouping_var (e.g., year) category left cross-validation iteration (specified out_var suffix '_out' lower case) computed statistic values training (rep_cv) true datasets (est_original) Error metrics: error (error), squared error (sq_error), absolute difference (abs_error), relative difference (rel_error), percent difference (perc_error) Error metrics summarised grouping_var: mean relative difference (mre), mean squared error (mse) root mean squared error (rmse) See Details section error metrics calculated.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/cross_validate_cube.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Leave-one-out cross-validation for data cubes — cross_validate_cube","text":"function assesses influence category out_var indicator value iteratively leaving one category time, similar leave-one-cross-validation. K-fold CV works similar fashion experimental covered . Original Sample Data: \\(\\mathbf{X} = \\{X_{11}, X_{12}, X_{13}, \\ldots, X_{sn}\\}\\) initial set data points, \\(s\\) different categories out_var \\(n\\) total samples across categories (= sample size). \\(n\\) corresponds number cells data cube number rows tabular format. Statistic Interest: \\(\\theta\\) parameter statistic estimated, mean \\(\\bar{X}\\), variance \\(\\sigma^2\\), biodiversity indicator. Let \\(\\hat{\\theta}\\) denote estimated value \\(\\theta\\) calculated complete dataset \\(\\mathbf{X}\\). Cross-Validation (CV) Sample: \\(\\mathbf{X}_{-s_j}\\) full dataset \\(\\mathbf{X}\\) excluding samples belonging category \\(j\\). subset used investigate influence category \\(j\\) estimated statistic \\(\\hat{\\theta}\\). CV Estimate Category \\(\\mathbf{j}\\): \\(\\hat{\\theta}_{-s_j}\\) value statistic interest calculated \\(\\mathbf{X}_{-s_j}\\), excludes category \\(j\\). example, \\(\\theta\\) sample mean, \\(\\hat{\\theta}_{-s_j} = \\bar{X}_{-s_j}\\). Error Measures: Error difference statistic estimated without category \\(j\\) (\\(\\hat{\\theta}_{-s_j}\\)) statistic calculated complete dataset (\\(\\hat{\\theta}\\)). $$\\text{Error}_{s_j} = \\hat{\\theta}_{-s_j} - \\hat{\\theta}$$ Relative Error absolute error, normalised true estimate \\(\\hat{\\theta}\\) small error term \\(\\epsilon = 10^{-8}\\) avoid division zero. $$\\text{Rel. Error}_{s_j} = \\frac{|\\hat{\\theta}_{-s_j} - \\hat{\\theta}|}{\\hat{\\theta} +\\epsilon}$$ Percent Error relative error expressed percentage. $$\\text{Perc. Error}_{s_j} = \\text{Rel. Error}_{s_j} \\times 100 \\%$$ Summary Measures: Mean Relative Error (MRE) average relative errors categories. $$\\text{MRE} = \\frac{1}{s} \\sum_{j=1}^s \\text{Rel. Error}_{s_j}$$ Mean Squared Error (MSE) average squared errors. $$\\text{MSE} = \\frac{1}{s} \\sum_{j=1}^s (\\text{Error}_{s_j})^2$$ Root Mean Squared Error (RMSE) square root MSE. $$\\text{RMSE} = \\sqrt{\\text{MSE}}$$","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/cross_validate_cube.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Leave-one-out cross-validation for data cubes — cross_validate_cube","text":"","code":"if (FALSE) { # \\dontrun{ # After processing a data cube with b3gbi::process_cube()  # Function to calculate statistic of interest # Mean observations per year mean_obs <- function(data) {   out_df <- aggregate(obs ~ year, data, mean) # Calculate mean obs per year   names(out_df) <- c(\"year\", \"diversity_val\") # Rename columns   return(out_df) } mean_obs(processed_cube$data)  # Perform leave-one-species-out CV cv_mean_obs <- cross_validate_cube(   data_cube = processed_cube,   fun = mean_obs,   grouping_var = \"year\",   out_var = \"taxonKey\",   crossv_method = \"loo\",   progress = FALSE ) head(cv_mean_obs) } # }"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/norm_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate normal bootstrap confidence interval — norm_ci","title":"Calculate normal bootstrap confidence interval — norm_ci","text":"function calculates normal confidence interval bootstrap sample. used calculate_bootstrap_ci().","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/norm_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate normal bootstrap confidence interval — norm_ci","text":"","code":"norm_ci(   t0,   t,   conf = 0.95,   h = function(t) t,   hinv = function(t) t,   no_bias = FALSE )"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/norm_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate normal bootstrap confidence interval — norm_ci","text":"t0 Original statistic. t Numeric vector bootstrap replicates. conf numeric value specifying confidence level interval. Default 0.95 (95 % confidence level). h function defining transformation. intervals calculated scale h(t) inverse function hinv applied resulting intervals. must function one variable . default identity function. hinv function, like h, returns inverse h. used transform intervals calculated scale h(t) back original scale. default identity function. h supplied hinv , intervals returned transformed scale. no_bias Logical. TRUE intervals centered around original estimates (bias ignored). Default FALSE.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/norm_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate normal bootstrap confidence interval — norm_ci","text":"matrix four columns: conf: confidence level ll: lower confidence limit ul: lower confidence limit","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/norm_ci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate normal bootstrap confidence interval — norm_ci","text":"$$CI_{norm} = \\left[\\hat{\\theta} - \\text{Bias}_{\\text{boot}} - \\text{SE}_{\\text{boot}} \\times z_{1-\\alpha/2}, \\hat{\\theta} - \\text{Bias}_{\\text{boot}} + \\text{SE}_{\\text{boot}} \\times z_{1-\\alpha/2} \\right]$$ \\(z_{1-\\alpha/2}\\) \\(1-\\alpha/2\\) quantile standard normal distribution.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/norm_ci.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Calculate normal bootstrap confidence interval — norm_ci","text":"function adapted function norm.ci() boot package (Canty & Ripley, 1999).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/norm_ci.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate normal bootstrap confidence interval — norm_ci","text":"Canty, ., & Ripley, B. (1999). boot: Bootstrap Functions (Originally Angelo Canty S) [Computer software]. https://CRAN.R-project.org/package=boot Davison, . C., & Hinkley, D. V. (1997). Bootstrap Methods Application (1st ed.). Cambridge University Press. doi:10.1017/CBO9780511802843","code":""},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/reference/norm_ci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate normal bootstrap confidence interval — norm_ci","text":"","code":"set.seed(123) boot_reps <- rnorm(1000) t0 <- mean(boot_reps)  # Normal-based CI norm_ci(t0, boot_reps, conf = 0.90) #>      conf        ll       ul #> [1,]  0.9 -1.615065 1.647321  # Without bias correction norm_ci(t0, boot_reps, conf = 0.90, no_bias = TRUE) #>      conf        ll       ul #> [1,]  0.9 -1.615065 1.647321"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/perc_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate percentile bootstrap confidence interval — perc_ci","title":"Calculate percentile bootstrap confidence interval — perc_ci","text":"function calculates percentile confidence interval bootstrap sample. used calculate_bootstrap_ci().","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/perc_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate percentile bootstrap confidence interval — perc_ci","text":"","code":"perc_ci(t, conf = 0.95, h = function(t) t, hinv = function(t) t)"},{"path":"https://b-cubed-eu.github.io/dubicube/reference/perc_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate percentile bootstrap confidence interval — perc_ci","text":"t Numeric vector bootstrap replicates. conf numeric value specifying confidence level interval. Default 0.95 (95 % confidence level). h function defining transformation. intervals calculated scale h(t) inverse function hinv applied resulting intervals. must function one variable . default identity function. hinv function, like h, returns inverse h. used transform intervals calculated scale h(t) back original scale. default identity function. h supplied hinv , intervals returned transformed scale.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/perc_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate percentile bootstrap confidence interval — perc_ci","text":"matrix four columns: conf: confidence level rk_lower: rank lower endpoint (interpolated) rk_upper: rank upper endpoint (interpolated) ll: lower confidence limit ul: lower confidence limit","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/perc_ci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate percentile bootstrap confidence interval — perc_ci","text":"$$CI_{perc} = \\left[ \\hat{\\theta}^*_{(\\alpha/2)}, \\hat{\\theta}^*_{(1-\\alpha/2)} \\right]$$ \\(\\hat{\\theta}^*_{(\\alpha/2)}\\) \\(\\hat{\\theta}^*_{(1-\\alpha/2)}\\) \\(\\alpha/2\\) \\(1-\\alpha/2\\) percentiles bootstrap distribution, respectively.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/perc_ci.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Calculate percentile bootstrap confidence interval — perc_ci","text":"function adapted internal function perc.ci() boot package (Canty & Ripley, 1999).","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/reference/perc_ci.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate percentile bootstrap confidence interval — perc_ci","text":"Canty, ., & Ripley, B. (1999). boot: Bootstrap Functions (Originally Angelo Canty S) [Computer software]. https://CRAN.R-project.org/package=boot Davison, . C., & Hinkley, D. V. (1997). Bootstrap Methods Application (1st ed.). Cambridge University Press. doi:10.1017/CBO9780511802843","code":""},{"path":[]},{"path":"https://b-cubed-eu.github.io/dubicube/reference/perc_ci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate percentile bootstrap confidence interval — perc_ci","text":"","code":"set.seed(123) boot_reps <- rnorm(1000)      # bootstrap replicates t0 <- mean(boot_reps)         # observed statistic  # Percentile CI perc_ci(boot_reps, conf = 0.95) #>      conf rk_lower rk_upper       ll       ul #> [1,] 0.95    25.03   975.98 -1.94292 2.049767"},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-095","dir":"Changelog","previous_headings":"","what":"dubicube 0.9.5","title":"dubicube 0.9.5","text":"Drop levels occur calc_bootstrap_ci() #69 Clarify whole-cube bootstrap versus group-specific bootstrap #70","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-094","dir":"Changelog","previous_headings":"","what":"dubicube 0.9.4","title":"dubicube 0.9.4","text":"use internal functions boot package #56","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-093","dir":"Changelog","previous_headings":"","what":"dubicube 0.9.3","title":"dubicube 0.9.3","text":"Review tutorials","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-092","dir":"Changelog","previous_headings":"","what":"dubicube 0.9.2","title":"dubicube 0.9.2","text":"Update README #63","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-091","dir":"Changelog","previous_headings":"","what":"dubicube 0.9.1","title":"dubicube 0.9.1","text":"Fix mistake package versioning","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-083","dir":"Changelog","previous_headings":"","what":"dubicube 0.8.3","title":"dubicube 0.8.3","text":"Fix mistake installation guidelines","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-082","dir":"Changelog","previous_headings":"","what":"dubicube 0.8.2","title":"dubicube 0.8.2","text":"Create tutorial visualising uncertainty case temporal indicators Create tutorial visualising uncertainty case spatial indicators","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-081","dir":"Changelog","previous_headings":"","what":"dubicube 0.8.1","title":"dubicube 0.8.1","text":"Silence package warnings vignettes Set repo status active","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-080","dir":"Changelog","previous_headings":"","what":"dubicube 0.8.0","title":"dubicube 0.8.0","text":"Simplify compatibility b3gbi package #48 Add links tutorials README #49 Make README attractive #50 Improve table layout tutorial effect classification #51 Include bootstrap figure tutorial #37","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-073","dir":"Changelog","previous_headings":"","what":"dubicube 0.7.3","title":"dubicube 0.7.3","text":"Update README #39 #45 Create tutorial effect classification","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-072","dir":"Changelog","previous_headings":"","what":"dubicube 0.7.2","title":"dubicube 0.7.2","text":"Use ROR copyright holder, drop email #40 Use DOI funding #41 Redo spelling check checklist v0.4.2 #42","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-071","dir":"Changelog","previous_headings":"","what":"dubicube 0.7.1","title":"dubicube 0.7.1","text":"Fix bug pkgdown site","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-070","dir":"Changelog","previous_headings":"","what":"dubicube 0.7.0","title":"dubicube 0.7.0","text":"Add tutorials #30 Simplify function examples Check package lintr::indentation_linter()","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-060","dir":"Changelog","previous_headings":"","what":"dubicube 0.6.0","title":"dubicube 0.6.0","text":"Add transformation bias options calculate_bootstrap_ci()","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-050","dir":"Changelog","previous_headings":"","what":"dubicube 0.5.0","title":"dubicube 0.5.0","text":"Move acceleration calculation separate function calculate_acceleration() Improve structure bootstrap_cube() function","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-040","dir":"Changelog","previous_headings":"","what":"dubicube 0.4.0","title":"dubicube 0.4.0","text":"Allow grouping_var argument length > 1 Implement ellipsis argument ... cross_validate_cube()","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-031","dir":"Changelog","previous_headings":"","what":"dubicube 0.3.1","title":"dubicube 0.3.1","text":"Fix issues: #19, #22, #23","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-030","dir":"Changelog","previous_headings":"","what":"dubicube 0.3.0","title":"dubicube 0.3.0","text":"Add cross-validation function","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-021","dir":"Changelog","previous_headings":"","what":"dubicube 0.2.1","title":"dubicube 0.2.1","text":"Join R-universe!","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-020","dir":"Changelog","previous_headings":"","what":"dubicube 0.2.0","title":"dubicube 0.2.0","text":"Add effect classification function Add ... argument bootstrap_cube() calculate_bootstrap_ci()","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-010","dir":"Changelog","previous_headings":"","what":"dubicube 0.1.0","title":"dubicube 0.1.0","text":"Add bootstrapping functions","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-002","dir":"Changelog","previous_headings":"","what":"dubicube 0.0.2","title":"dubicube 0.0.2","text":"Add pkgdown website. Add package description README.Rmd.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-001","dir":"Changelog","previous_headings":"","what":"dubicube 0.0.1","title":"dubicube 0.0.1","text":"Set basic package structure.","code":""},{"path":"https://b-cubed-eu.github.io/dubicube/news/index.html","id":"dubicube-000","dir":"Changelog","previous_headings":"","what":"dubicube 0.0.0","title":"dubicube 0.0.0","text":"Added NEWS.md file track changes package. Add checklist infrastructure.","code":""}]
